Fix char16_t (u"") string literal support
==========================================

Problem: u"..." strings (char16_t / UTF-16) are currently treated as narrow
strings by the lexer (StringLiteral token), when they should produce 2-byte
per-element string data. This causes multiple test failures where char16_t
arrays are incorrectly sized and contain wrong data.

Plan:
1. Add Char16StringLiteral token type in lexer
2. Lex u"..." as Char16StringLiteral (distinct from L"..." / U"..." which are WideStringLiteral)
3. Add Expr::Char16StringLiteral AST node in parser
4. Handle u16 string concatenation properly (u"a" u"b" -> u"ab", "a" u"b" -> u"ab")
5. Type char16_t strings as unsigned short * (uint16_t)
6. Add IR representation: Vec<u16> in char16_string_literals
7. Lower char16_t strings to IR (interning, alloca emission, global init)
8. Emit .short values in backend data sections

This fixes compiler_suite_0057_0080 (test2), compiler_suite_0057_0082 (test1),
and potentially other tests involving char16_t and u"" string literals.

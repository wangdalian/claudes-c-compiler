use crate::frontend::parser::ast::*;
use crate::ir::ir::*;
use crate::common::types::{AddressSpace, IrType, CType, StructLayout};
use super::lowering::Lowerer;
use super::definitions::{LocalInfo, GlobalInfo, DeclAnalysis, SwitchFrame, FuncSig, LValue};
use crate::frontend::sema::type_context::extract_fptr_typedef_info;
use crate::backend::inline_asm::{constraint_has_immediate_alt, constraint_is_memory_only};

impl Lowerer {
    pub(super) fn lower_compound_stmt(&mut self, compound: &CompoundStmt) {
        // If this block has __label__ declarations, push a local label scope
        // that maps each declared name to a unique scope-qualified name.
        let has_local_labels = !compound.local_labels.is_empty();
        if has_local_labels {
            let scope_id = self.next_local_label_scope;
            self.next_local_label_scope += 1;
            let mut scope = crate::common::fx_hash::FxHashMap::default();
            for name in &compound.local_labels {
                scope.insert(name.clone(), format!("{}$ll{}", name, scope_id));
            }
            self.local_label_scopes.push(scope);
        }

        // Check if this block contains any declarations. If not, we can skip
        // scope tracking entirely since statements don't introduce new bindings.
        let has_declarations = compound.items.iter().any(|item| matches!(item, BlockItem::Declaration(_)));

        if has_declarations {
            // Push a scope frame to track additions/modifications in this block.
            // On scope exit, we undo changes instead of cloning entire HashMaps.
            self.push_scope();

            for item in &compound.items {
                match item {
                    BlockItem::Declaration(decl) => {
                        self.collect_enum_constants_scoped(&decl.type_spec);
                        self.lower_local_decl(decl);
                    }
                    BlockItem::Statement(stmt) => self.lower_stmt(stmt),
                }
            }

            self.pop_scope();
        } else {
            // No declarations: just lower statements without scope overhead.
            for item in &compound.items {
                if let BlockItem::Statement(stmt) = item {
                    self.lower_stmt(stmt);
                }
            }
        }

        // Pop the local label scope if we pushed one
        if has_local_labels {
            self.local_label_scopes.pop();
        }
    }

    pub(super) fn lower_local_decl(&mut self, decl: &Declaration) {
        // Resolve typeof(expr) or __auto_type to concrete type before processing
        let resolved_decl;
        let decl = if matches!(&decl.type_spec, TypeSpecifier::Typeof(_) | TypeSpecifier::TypeofType(_) | TypeSpecifier::AutoType) {
            let resolved_type_spec = if matches!(&decl.type_spec, TypeSpecifier::AutoType) {
                // __auto_type: infer type from the first declarator's initializer
                if let Some(first) = decl.declarators.first() {
                    if let Some(Initializer::Expr(ref init_expr)) = first.init {
                        if let Some(ctype) = self.get_expr_ctype(init_expr) {
                            Self::ctype_to_type_spec(&ctype)
                        } else {
                            TypeSpecifier::Int // fallback
                        }
                    } else {
                        TypeSpecifier::Int // fallback: no initializer
                    }
                } else {
                    TypeSpecifier::Int
                }
            } else {
                self.resolve_typeof(&decl.type_spec)
            };
            resolved_decl = Declaration {
                type_spec: resolved_type_spec,
                declarators: decl.declarators.clone(),
                is_typedef: decl.is_typedef,
                is_static: decl.is_static,
                is_extern: decl.is_extern,
                is_const: decl.is_const,
                is_volatile: decl.is_volatile,
                is_common: decl.is_common,
                is_transparent_union: decl.is_transparent_union,
                alignment: decl.alignment,
                span: decl.span,
            };
            &resolved_decl
        } else {
            decl
        };

        self.register_struct_type(&decl.type_spec);

        if decl.is_typedef {
            for declarator in &decl.declarators {
                if !declarator.name.is_empty() {
                    if let Some(fti) = extract_fptr_typedef_info(&decl.type_spec, &declarator.derived) {
                        self.types.func_ptr_typedefs.insert(declarator.name.clone());
                        self.types.func_ptr_typedef_info.insert(declarator.name.clone(), fti);
                    }
                    let resolved_ctype = self.build_full_ctype(&decl.type_spec, &declarator.derived);
                    self.types.insert_typedef_scoped(declarator.name.clone(), resolved_ctype);

                    // For VLA typedefs (e.g., `typedef char buf[n][m]`), compute the
                    // runtime sizeof and store it so that `sizeof(buf)` can use it.
                    if self.func_state.is_some() {
                        if let Some(vla_size) = self.compute_vla_runtime_size(&decl.type_spec, &declarator.derived) {
                            self.func_mut().insert_vla_typedef_size_scoped(declarator.name.clone(), vla_size);
                        }
                    }
                }
            }
            return;
        }

        for declarator in &decl.declarators {
            if declarator.name.is_empty() {
                continue;
            }

            // Extern declarations reference a global symbol, not a local variable
            if decl.is_extern {
                if self.lower_extern_decl(decl, declarator) {
                    continue;
                }
                // Fall through to function declaration handler if it was an extern func decl
            }

            // Block-scope function declarations: `int f(int);` or typedef-based `func_t add;`
            if self.try_lower_block_func_decl(decl, declarator) {
                continue;
            }

            // Shared declaration analysis
            let mut da = self.analyze_declaration(&decl.type_spec, &declarator.derived);
            self.fixup_unsized_array(&mut da, &decl.type_spec, &declarator.derived, &declarator.init);

            // Detect complex type variables and arrays of complex elements
            let is_complex = !da.is_pointer && !da.is_array && self.is_type_complex(&decl.type_spec);
            let complex_elem_ctype: Option<CType> = if da.is_array && !da.is_pointer {
                let ctype = self.type_spec_to_ctype(&decl.type_spec);
                match ctype {
                    CType::ComplexFloat => Some(CType::ComplexFloat),
                    CType::ComplexDouble => Some(CType::ComplexDouble),
                    CType::ComplexLongDouble => Some(CType::ComplexLongDouble),
                    _ => None,
                }
            } else {
                None
            };

            if decl.is_static {
                // For local static arrays-of-pointers, clear struct_layout so
                // lower_global_init uses the pointer-array path (Compound with
                // relocations) instead of the struct byte-serialization path.
                // This matches the file-scope global handling in lower_top_level_declaration.
                if da.is_array_of_pointers || da.is_array_of_func_ptrs {
                    da.struct_layout = None;
                    da.is_struct = false;
                }
                self.lower_local_static_decl(&decl, &declarator, &da);
                continue;
            }

            let vla_size = if da.is_array {
                self.compute_vla_runtime_size(&decl.type_spec, &declarator.derived)
            } else {
                None
            };

            let alloca;
            if let Some(vla_size_val) = vla_size {
                // VLA: allocate dynamically on the stack using DynAlloca.
                // VLAs must stay at the declaration point (not hoisted) because
                // their size is computed at runtime.
                alloca = self.fresh_value();
                // First, save the stack pointer if this is the first VLA in the function.
                if !self.func().has_vla {
                    self.func_mut().has_vla = true;
                    let save_val = self.fresh_value();
                    self.emit(Instruction::StackSave { dest: save_val });
                    self.func_mut().vla_stack_save = Some(save_val);
                }
                let align = decl.alignment.unwrap_or(16).max(16);
                self.emit(Instruction::DynAlloca {
                    dest: alloca,
                    size: Operand::Value(vla_size_val),
                    align,
                });
            } else {
                // Hoist static-size allocas to the entry block so that variables
                // whose declarations are skipped by `goto` still have valid
                // stack slots at runtime.
                alloca = self.emit_entry_alloca(
                    if da.is_array || da.is_struct || is_complex { IrType::Ptr } else { da.var_ty },
                    da.actual_alloc_size,
                    decl.alignment.unwrap_or(0),
                    decl.is_volatile,
                );
            }
            let mut local_info = LocalInfo::from_analysis(&da, alloca);
            local_info.vla_size = vla_size;
            // For local VLAs with multiple dimensions, compute runtime strides
            // so that subscript operations use the correct element sizes.
            if vla_size.is_some() {
                let strides = self.compute_vla_local_strides(&decl.type_spec, &declarator.derived);
                if !strides.is_empty() {
                    local_info.vla_strides = strides;
                }
            }
            local_info.asm_register = declarator.asm_register.clone();
            self.insert_local_scoped(declarator.name.clone(), local_info);

            // Track function pointer return and param types
            for d in &declarator.derived {
                if let DerivedDeclarator::FunctionPointer(params, _) = d {
                    let ret_ty = self.type_spec_to_ir(&decl.type_spec);
                    let param_tys: Vec<IrType> = params.iter().map(|p| {
                        self.type_spec_to_ir(&p.type_spec)
                    }).collect();
                    self.func_meta.ptr_sigs.insert(declarator.name.clone(), FuncSig::for_ptr(ret_ty, param_tys));
                    break;
                }
            }

            if let Some(ref init) = declarator.init {
                match init {
                    Initializer::Expr(expr) => {
                        // Track const values before lowering (needed for VLA sizes)
                        if decl.is_const && !da.is_pointer && !da.is_array && !da.is_struct && !is_complex {
                            if let Some(const_val) = self.eval_const_expr(expr) {
                                if let Some(ival) = self.const_to_i64(&const_val) {
                                    self.insert_const_local_scoped(declarator.name.clone(), ival);
                                }
                            }
                        }
                        self.lower_local_init_expr(expr, alloca, &da, is_complex, decl);
                    }
                    Initializer::List(items) => {
                        self.lower_local_init_list(
                            items, alloca, &da, is_complex, &complex_elem_ctype,
                            decl, &declarator.name,
                        );
                    }
                }
            }
        }
    }

    /// Handle static local variable declarations: emit as globals with mangled names.
    /// Static locals are initialized once at program start (via .data/.bss),
    /// not at every function call.
    fn lower_local_static_decl(&mut self, decl: &Declaration, declarator: &InitDeclarator, da: &DeclAnalysis) {
        let static_id = self.next_static_local;
        let static_name = format!("{}.{}.{}", self.func_mut().name, declarator.name, static_id);

        // Register the bare name -> mangled name mapping before processing the initializer
        // so that &x in another static's initializer can resolve to the mangled name.
        self.insert_static_local_scoped(declarator.name.clone(), static_name.clone());

        // Determine initializer (evaluated at compile time for static locals)
        let init = if let Some(ref initializer) = declarator.init {
            self.lower_global_init(
                initializer, &decl.type_spec, da.base_ty, da.is_array,
                da.elem_size, da.actual_alloc_size, &da.struct_layout, &da.array_dim_strides,
            )
        } else {
            GlobalInit::Zero
        };

        // Respect explicit __attribute__((aligned(N))) / _Alignas(N) on static locals
        let align = if let Some(explicit) = decl.alignment {
            da.var_ty.align().max(explicit)
        } else {
            da.var_ty.align()
        };

        // For struct initializers emitted as byte arrays, set element type to I8
        let global_ty = da.resolve_global_ty(&init);

        self.emitted_global_names.insert(static_name.clone());
        self.module.globals.push(IrGlobal {
            name: static_name.clone(),
            ty: global_ty,
            size: da.actual_alloc_size,
            align,
            init,
            is_static: true,
            is_extern: false,
            is_common: false,
            section: None,
        });

        // Track as a global for access via GlobalAddr
        self.globals.insert(static_name.clone(), GlobalInfo::from_analysis(da));

        // Store type info in locals (with static_global_name set so each use site
        // emits a fresh GlobalAddr in its own basic block, avoiding unreachable-block issues).
        self.insert_local_scoped(declarator.name.clone(), LocalInfo::for_static(da, static_name));
        self.next_static_local += 1;
    }

    /// Lower a {real, imag} list initializer for a complex field.
    /// Stores the real and imaginary parts at dest_addr and dest_addr+comp_size.
    pub(super) fn lower_complex_list_init(
        &mut self,
        sub_items: &[InitializerItem],
        dest_addr: Value,
        complex_ctype: &CType,
    ) {
        let comp_ty = Self::complex_component_ir_type(complex_ctype);
        let comp_size = Self::complex_component_size(complex_ctype);
        // Store real part
        if let Some(first) = sub_items.first() {
            if let Initializer::Expr(e) = &first.init {
                let val = self.lower_expr(e);
                let expr_ty = self.get_expr_type(e);
                let val = self.emit_implicit_cast(val, expr_ty, comp_ty);
                self.emit(Instruction::Store { val, ptr: dest_addr, ty: comp_ty , seg_override: AddressSpace::Default });
            }
        } else {
            let zero = Self::complex_zero(comp_ty);
            self.emit(Instruction::Store { val: zero, ptr: dest_addr, ty: comp_ty , seg_override: AddressSpace::Default });
        }
        // Store imag part
        let imag_ptr = self.emit_gep_offset(dest_addr, comp_size, IrType::I8);
        if let Some(item) = sub_items.get(1) {
            if let Initializer::Expr(e) = &item.init {
                let val = self.lower_expr(e);
                let expr_ty = self.get_expr_type(e);
                let val = self.emit_implicit_cast(val, expr_ty, comp_ty);
                self.emit(Instruction::Store { val, ptr: imag_ptr, ty: comp_ty , seg_override: AddressSpace::Default });
            }
        } else {
            let zero = Self::complex_zero(comp_ty);
            self.emit(Instruction::Store { val: zero, ptr: imag_ptr, ty: comp_ty , seg_override: AddressSpace::Default });
        }
    }

    /// Emit a complex expression to a memory location at the given offset.
    /// Handles integer-to-complex conversion properly by using lower_expr_to_complex
    /// and then memcpy-ing the result to the destination.
    pub(super) fn emit_complex_expr_to_offset(&mut self, expr: &Expr, base_alloca: Value, offset: usize, complex_ctype: &CType) {
        let complex_size = complex_ctype.size();
        let src = self.lower_expr_to_complex(expr, complex_ctype);
        let dest_addr = self.emit_gep_offset(base_alloca, offset, IrType::Ptr);
        self.emit(Instruction::Memcpy {
            dest: dest_addr,
            src,
            size: complex_size,
        });
    }

    /// Lower an expression to a complex value, converting if needed.
    /// Returns a Value (pointer to the complex {real, imag} pair).
    pub(super) fn lower_expr_to_complex(&mut self, expr: &Expr, target_ctype: &CType) -> Value {
        let expr_ctype = self.expr_ctype(expr);
        let val = self.lower_expr(expr);
        if expr_ctype.is_complex() {
            if expr_ctype != *target_ctype {
                let val_v = self.operand_to_value(val);
                let converted = self.complex_to_complex(val_v, &expr_ctype, target_ctype);
                self.operand_to_value(converted)
            } else {
                self.operand_to_value(val)
            }
        } else {
            let converted = self.real_to_complex(val, &expr_ctype, target_ctype);
            self.operand_to_value(converted)
        }
    }

    /// Initialize a local struct from an initializer list.
    /// `base` is the base address of the struct in memory.
    pub(super) fn lower_local_struct_init(
        &mut self,
        items: &[InitializerItem],
        base: Value,
        layout: &StructLayout,
    ) {
        use crate::common::types::InitFieldResolution;
        use super::global_init_helpers as h;
        let mut current_field_idx = 0usize;
        let mut item_idx = 0usize;
        while item_idx < items.len() {
            let item = &items[item_idx];
            let desig_name = h::first_field_designator(item);
            let resolution = match layout.resolve_init_field(desig_name, current_field_idx, &self.types) {
                Some(r) => r,
                None => break,
            };

            // Handle anonymous member: drill into the anonymous struct/union
            let field_idx = match &resolution {
                InitFieldResolution::Direct(idx) => {
                    let f = &layout.fields[*idx];
                    // For positional init, if this is an anonymous struct/union member,
                    // drill into it and consume multiple init items for inner fields.
                    if desig_name.is_none() && f.name.is_empty() && f.bit_width.is_none() {
                        if let CType::Struct(key) | CType::Union(key) = &f.ty {
                            if let Some(sub_layout) = self.types.struct_layouts.get(&**key).cloned() {
                                let anon_offset = f.offset;
                                let sub_base = self.emit_gep_offset(base, anon_offset, IrType::Ptr);
                                let anon_field_count = sub_layout.fields.iter()
                                    .filter(|ff| !ff.name.is_empty() || ff.bit_width.is_none())
                                    .count();
                                let remaining = &items[item_idx..];
                                let consume_count = remaining.len().min(anon_field_count);
                                self.lower_local_struct_init(&remaining[..consume_count], sub_base, &sub_layout);
                                item_idx += consume_count;
                                current_field_idx = *idx + 1;
                                continue;
                            }
                        }
                    }
                    *idx
                }
                InitFieldResolution::AnonymousMember { anon_field_idx, inner_name } => {
                    if let Some(res) = h::resolve_anonymous_member(layout, *anon_field_idx, inner_name, &item.init, &self.types.struct_layouts) {
                        let sub_base = self.emit_gep_offset(base, res.anon_offset, IrType::Ptr);
                        self.lower_local_struct_init(&[res.sub_item], sub_base, &res.sub_layout);
                    }
                    current_field_idx = *anon_field_idx + 1;
                    item_idx += 1;
                    continue;
                }
            };

            let field = &layout.fields[field_idx];
            let field_offset = field.offset;

            // Complex fields need special handling: memcpy data instead of storing pointer
            if field.ty.is_complex() {
                let complex_ctype = field.ty.clone();
                let complex_size = complex_ctype.size();
                let dest_addr = self.emit_gep_offset(base, field_offset, IrType::Ptr);
                match &item.init {
                    Initializer::Expr(e) => {
                        let src = self.lower_expr_to_complex(e, &complex_ctype);
                        self.emit(Instruction::Memcpy {
                            dest: dest_addr,
                            src,
                            size: complex_size,
                        });
                    }
                    Initializer::List(sub_items) => {
                        self.lower_complex_list_init(sub_items, dest_addr, &complex_ctype);
                    }
                }
                current_field_idx = field_idx + 1;
                item_idx += 1;
                continue;
            }

            let field_ty = IrType::from_ctype(&field.ty);

            match &item.init {
                Initializer::Expr(e) => {
                    // For struct/union fields initialized from expressions that produce
                    // struct values (e.g., function calls returning structs via sret),
                    // we need to emit a memcpy rather than a scalar store, because
                    // lower_expr returns the sret alloca address (a pointer), not the
                    // struct data itself.
                    let is_struct_field = matches!(field.ty, CType::Struct(_) | CType::Union(_));
                    if is_struct_field && self.struct_value_size(e).is_some() {
                        let src_addr = self.get_struct_base_addr(e);
                        let field_size = self.resolve_ctype_size(&field.ty);
                        let field_addr = self.emit_gep_offset(base, field_offset, IrType::Ptr);
                        self.emit(Instruction::Memcpy {
                            dest: field_addr,
                            src: src_addr,
                            size: field_size,
                        });
                    } else if let CType::Array(ref elem_ty, Some(arr_size)) = field.ty {
                        // Char array field initialized by a string literal:
                        // copy the string bytes instead of storing the pointer.
                        let is_char_array = matches!(**elem_ty, CType::Char | CType::UChar);
                        if is_char_array {
                            if let Expr::StringLiteral(ref s, _) = e {
                                self.emit_string_to_alloca(base, s, field_offset);
                                // Zero-fill remaining bytes if string is shorter than array
                                let str_len = s.chars().count() + 1; // +1 for null terminator
                                for i in str_len..arr_size {
                                    let val = Operand::Const(IrConst::I8(0));
                                    self.emit_store_at_offset(base, field_offset + i, val, IrType::I8);
                                }
                            } else {
                                let val = self.lower_and_cast_init_expr(e, field_ty);
                                let field_addr = self.emit_gep_offset(base, field_offset, field_ty);
                                self.emit(Instruction::Store { val, ptr: field_addr, ty: field_ty , seg_override: AddressSpace::Default });
                            }
                        } else {
                            // Non-char array field with flat expression initializer:
                            // consume up to arr_size items from the init list to fill array elements
                            let elem_ir_ty = IrType::from_ctype(elem_ty);
                            let elem_size = elem_ir_ty.size().max(1);
                            let elem_is_bool = **elem_ty == CType::Bool;
                            let val = self.lower_init_expr_bool_aware(e, elem_ir_ty, elem_is_bool);
                            let field_addr = self.emit_gep_offset(base, field_offset, elem_ir_ty);
                            self.emit(Instruction::Store { val, ptr: field_addr, ty: elem_ir_ty , seg_override: AddressSpace::Default });
                            // Consume additional items for remaining array elements
                            let mut arr_idx = 1usize;
                            while arr_idx < arr_size && item_idx + 1 < items.len() {
                                item_idx += 1;
                                let next_item = &items[item_idx];
                                // Stop if we hit a designator (it targets a different field)
                                if !next_item.designators.is_empty() {
                                    item_idx -= 1; // put it back
                                    break;
                                }
                                if let Initializer::Expr(next_e) = &next_item.init {
                                    let next_val = self.lower_init_expr_bool_aware(next_e, elem_ir_ty, elem_is_bool);
                                    let offset = field_offset + arr_idx * elem_size;
                                    let elem_addr = self.emit_gep_offset(base, offset, elem_ir_ty);
                                    self.emit(Instruction::Store { val: next_val, ptr: elem_addr, ty: elem_ir_ty , seg_override: AddressSpace::Default });
                                }
                                arr_idx += 1;
                            }
                        }
                    } else {
                        let val = self.lower_init_expr_bool_aware(e, field_ty, field.ty == CType::Bool);
                        let field_addr = self.emit_gep_offset(base, field_offset, field_ty);
                        if let (Some(bit_offset), Some(bit_width)) = (field.bit_offset, field.bit_width) {
                            self.store_bitfield(field_addr, field_ty, bit_offset, bit_width, val);
                        } else {
                            self.emit(Instruction::Store { val, ptr: field_addr, ty: field_ty , seg_override: AddressSpace::Default });
                        }
                    }
                }
                Initializer::List(sub_items) => {
                    let field_addr = self.emit_gep_offset(base, field_offset, field_ty);
                    self.lower_struct_field_init_list(sub_items, field_addr, &field.ty);
                }
            }
            current_field_idx = field_idx + 1;
            item_idx += 1;
        }
    }

    /// Initialize a struct field from a nested initializer list.
    /// Handles sub-struct fields and array fields.
    fn lower_struct_field_init_list(
        &mut self,
        items: &[InitializerItem],
        base: Value,
        field_ctype: &CType,
    ) {
        match field_ctype {
            CType::Struct(key) | CType::Union(key) => {
                if let Some(sub_layout) = self.types.struct_layouts.get(&**key).cloned() {
                    self.lower_local_struct_init(items, base, &sub_layout);
                }
            }
            CType::Array(ref elem_ty, arr_size_opt) => {
                // Check for char array initialized by a brace-wrapped string literal:
                // e.g., struct field `char a[10]` initialized as `{"hello"}`
                let is_char_array = matches!(**elem_ty, CType::Char | CType::UChar);
                if is_char_array && items.len() == 1 {
                    if let Initializer::Expr(Expr::StringLiteral(ref s, _)) = items[0].init {
                        self.emit_string_to_alloca(base, s, 0);
                        // Zero-fill remaining bytes if string is shorter than array
                        if let Some(arr_size) = arr_size_opt {
                            let str_len = s.chars().count() + 1; // +1 for null terminator
                            for i in str_len..*arr_size {
                                let val = Operand::Const(IrConst::I8(0));
                                self.emit_store_at_offset(base, i, val, IrType::I8);
                            }
                        }
                        return;
                    }
                }
                // Array field: init elements with [idx]=val designator support
                let elem_ir_ty = IrType::from_ctype(elem_ty);
                let elem_size = self.resolve_ctype_size(elem_ty);
                let mut ai = 0usize;
                for item in items.iter() {
                    // Check for index designator: [idx]=val
                    if let Some(Designator::Index(ref idx_expr)) = item.designators.first() {
                        if let Some(idx) = self.eval_const_expr_for_designator(idx_expr) {
                            ai = idx;
                        }
                    }
                    let elem_is_bool = **elem_ty == CType::Bool;
                    match &item.init {
                        Initializer::Expr(e) => {
                            let val = self.lower_init_expr_bool_aware(e, elem_ir_ty, elem_is_bool);
                            self.emit_store_at_offset(base, ai * elem_size, val, elem_ir_ty);
                        }
                        Initializer::List(sub_items) => {
                            // Braced sub-initializer for array element (e.g., struct element)
                            let elem_addr = self.emit_gep_offset(base, ai * elem_size, elem_ir_ty);
                            self.lower_struct_field_init_list(sub_items, elem_addr, elem_ty);
                        }
                    }
                    ai += 1;
                }
            }
            _ => {
                // Scalar field with nested braces (e.g., int x = {5})
                if let Some(first) = items.first() {
                    if let Initializer::Expr(e) = &first.init {
                        let field_ir_ty = IrType::from_ctype(field_ctype);
                        let val = self.lower_init_expr_bool_aware(e, field_ir_ty, *field_ctype == CType::Bool);
                        self.emit(Instruction::Store { val, ptr: base, ty: field_ir_ty , seg_override: AddressSpace::Default });
                    }
                }
            }
        }
    }

    /// Unwrap nested `Initializer::List` items to find the innermost expression.
    /// Used for double-brace init like `{{expr}}` to extract the `expr`.
    pub(super) fn unwrap_nested_init_expr(items: &[InitializerItem]) -> Option<&Expr> {
        if let Some(first) = items.first() {
            match &first.init {
                Initializer::Expr(e) => Some(e),
                Initializer::List(sub_items) => Self::unwrap_nested_init_expr(sub_items),
            }
        } else {
            None
        }
    }

    /// Lower a multi-dimensional array initializer list.
    /// Handles nested braces like `{{1,2,3},{4,5,6}}` for `int a[2][3]`.
    pub(super) fn lower_array_init_list(
        &mut self,
        items: &[InitializerItem],
        alloca: Value,
        base_ty: IrType,
        array_dim_strides: &[usize],
    ) {
        let mut flat_index = 0usize;
        self.lower_array_init_recursive(items, alloca, base_ty, array_dim_strides, &mut flat_index);
    }

    /// Recursive helper for multi-dimensional array initialization.
    /// Processes each initializer item, recursing for nested braces and
    /// advancing the flat_index to track the current element position.
    fn lower_array_init_recursive(
        &mut self,
        items: &[InitializerItem],
        alloca: Value,
        base_ty: IrType,
        array_dim_strides: &[usize],
        flat_index: &mut usize,
    ) {
        let elem_size = *array_dim_strides.last().unwrap_or(&1);
        let base_type_size = base_ty.size().max(1);
        let sub_elem_count = if array_dim_strides.len() > 1 && elem_size > 0 {
            array_dim_strides[0] / elem_size
        } else {
            1
        };

        for item in items {
            // Check for multi-dimensional index designators: [i][j]...
            let index_designators: Vec<usize> = item.designators.iter().filter_map(|d| {
                if let Designator::Index(ref idx_expr) = d {
                    self.eval_const_expr_for_designator(idx_expr)
                } else {
                    None
                }
            }).collect();

            if !index_designators.is_empty() {
                // Compute flat scalar index from multi-dimensional indices
                let mut target_flat = 0usize;
                for (i, &idx) in index_designators.iter().enumerate() {
                    let elems_per_entry = if i < array_dim_strides.len() && base_type_size > 0 {
                        array_dim_strides[i] / base_type_size
                    } else {
                        1
                    };
                    target_flat += idx * elems_per_entry;
                }

                match &item.init {
                    Initializer::List(sub_items) => {
                        // Set flat_index to designated position and recurse
                        *flat_index = target_flat;
                        let remaining_dims = array_dim_strides.len().saturating_sub(index_designators.len());
                        let sub_strides = &array_dim_strides[array_dim_strides.len() - remaining_dims..];
                        if remaining_dims > 0 {
                            self.lower_array_init_recursive(sub_items, alloca, base_ty, sub_strides, flat_index);
                        } else {
                            for sub_item in sub_items {
                                if let Initializer::Expr(e) = &sub_item.init {
                                    let val = self.lower_and_cast_init_expr(e, base_ty);
                                    self.emit_array_element_store(alloca, val, *flat_index * elem_size, base_ty);
                                    *flat_index += 1;
                                }
                            }
                        }
                    }
                    Initializer::Expr(e) => {
                        if base_ty == IrType::I8 || base_ty == IrType::U8 {
                            if let Expr::StringLiteral(s, _) = e {
                                self.emit_string_to_alloca(alloca, s, target_flat * elem_size);
                                *flat_index = target_flat + sub_elem_count;
                                continue;
                            }
                        }
                        let val = self.lower_and_cast_init_expr(e, base_ty);
                        self.emit_array_element_store(alloca, val, target_flat * elem_size, base_ty);
                        *flat_index = target_flat + 1;
                    }
                }
                continue;
            }

            let start_index = *flat_index;
            match &item.init {
                Initializer::List(sub_items) => {
                    if array_dim_strides.len() > 1 {
                        self.lower_array_init_recursive(sub_items, alloca, base_ty, &array_dim_strides[1..], flat_index);
                    } else {
                        // Bottom level: treat as flat elements
                        for sub_item in sub_items {
                            if let Initializer::Expr(e) = &sub_item.init {
                                let val = self.lower_and_cast_init_expr(e, base_ty);
                                self.emit_array_element_store(alloca, val, *flat_index * elem_size, base_ty);
                                *flat_index += 1;
                            }
                        }
                    }
                    // Advance to next sub-array boundary after a braced sub-list
                    // This handles partial initialization: {{1,2},{3}} for int a[2][3]
                    // After {1,2} we advance to index 3 (next row), after {3} to index 6
                    if array_dim_strides.len() > 1 {
                        let boundary = start_index + sub_elem_count;
                        if *flat_index < boundary {
                            *flat_index = boundary;
                        }
                    }
                }
                Initializer::Expr(e) => {
                    // String literal fills a sub-array in char arrays
                    if base_ty == IrType::I8 || base_ty == IrType::U8 {
                        if let Expr::StringLiteral(s, _) = e {
                            self.emit_string_to_alloca(alloca, s, *flat_index * elem_size);
                            let string_stride = if array_dim_strides.len() >= 2 {
                                array_dim_strides[array_dim_strides.len() - 2]
                            } else {
                                sub_elem_count
                            };
                            *flat_index += string_stride;
                            continue;
                        }
                    }
                    // Bare scalar: fills one base element without sub-array padding
                    let val = self.lower_and_cast_init_expr(e, base_ty);
                    self.emit_array_element_store(alloca, val, *flat_index * elem_size, base_ty);
                    *flat_index += 1;
                }
            }
        }
    }

    /// Lower an expression and cast it to the target type (e.g., int to float).
    pub(super) fn lower_and_cast_init_expr(&mut self, expr: &Expr, target_ty: IrType) -> Operand {
        let val = self.lower_expr(expr);
        let expr_ty = self.get_expr_type(expr);
        self.emit_implicit_cast(val, expr_ty, target_ty)
    }

    /// Lower an init expression with bool-aware handling.
    /// For bool targets, normalizes to 0/1. For other types, casts to target_ty.
    pub(super) fn lower_init_expr_bool_aware(&mut self, expr: &Expr, target_ty: IrType, is_bool: bool) -> Operand {
        if is_bool {
            let val = self.lower_expr(expr);
            let expr_ty = self.get_expr_type(expr);
            self.emit_bool_normalize_typed(val, expr_ty)
        } else {
            self.lower_and_cast_init_expr(expr, target_ty)
        }
    }

    /// Main statement lowering dispatcher. Delegates to per-statement-type methods.
    pub(super) fn lower_stmt(&mut self, stmt: &Stmt) {
        match stmt {
            Stmt::Return(expr, _span) => {
                let op = expr.as_ref().map(|e| self.lower_return_expr(e));
                self.terminate(Terminator::Return(op));
                let label = self.fresh_label();
                self.start_block(label);
            }
            Stmt::Expr(Some(expr)) => { self.lower_expr(expr); }
            Stmt::Expr(None) => {}
            Stmt::Compound(compound) => self.lower_compound_stmt(compound),
            Stmt::If(cond, then_stmt, else_stmt, _span) => self.lower_if_stmt(cond, then_stmt, else_stmt.as_deref()),
            Stmt::While(cond, body, _span) => self.lower_while_stmt(cond, body),
            Stmt::For(init, cond, inc, body, _span) => self.lower_for_stmt(init, cond, inc, body),
            Stmt::DoWhile(body, cond, _span) => self.lower_do_while_stmt(body, cond),
            Stmt::Break(_span) => self.lower_break_stmt(),
            Stmt::Continue(_span) => self.lower_continue_stmt(),
            Stmt::Switch(expr, body, _span) => self.lower_switch_stmt(expr, body),
            Stmt::Case(expr, stmt, _span) => self.lower_case_stmt(expr, stmt),
            Stmt::CaseRange(low_expr, high_expr, stmt, _span) => self.lower_case_range_stmt(low_expr, high_expr, stmt),
            Stmt::Default(stmt, _span) => self.lower_default_stmt(stmt),
            Stmt::Goto(label, _span) => self.lower_goto_stmt(label),
            Stmt::GotoIndirect(expr, _span) => self.lower_goto_indirect_stmt(expr),
            Stmt::Label(name, stmt, _span) => self.lower_label_stmt(name, stmt),
            Stmt::InlineAsm { template, outputs, inputs, clobbers, goto_labels } => {
                self.lower_inline_asm_stmt(template, outputs, inputs, clobbers, goto_labels);
            }
        }
    }

    fn lower_if_stmt(&mut self, cond: &Expr, then_stmt: &Stmt, else_stmt: Option<&Stmt>) {
        let cond_val = self.lower_condition_expr(cond);
        let then_label = self.fresh_label();
        let else_label = self.fresh_label();
        let end_label = self.fresh_label();

        let false_target = if else_stmt.is_some() { else_label } else { end_label };
        self.terminate(Terminator::CondBranch {
            cond: cond_val,
            true_label: then_label,
            false_label: false_target,
        });

        self.start_block(then_label);
        self.lower_stmt(then_stmt);
        self.terminate(Terminator::Branch(end_label));

        if let Some(else_stmt) = else_stmt {
            self.start_block(else_label);
            self.lower_stmt(else_stmt);
            self.terminate(Terminator::Branch(end_label));
        }

        self.start_block(end_label);
    }

    fn lower_while_stmt(&mut self, cond: &Expr, body: &Stmt) {
        let cond_label = self.fresh_label();
        let body_label = self.fresh_label();
        let end_label = self.fresh_label();

        self.func_mut().break_labels.push(end_label);
        self.func_mut().continue_labels.push(cond_label);

        self.terminate(Terminator::Branch(cond_label));

        self.start_block(cond_label);
        let cond_val = self.lower_condition_expr(cond);
        self.terminate(Terminator::CondBranch {
            cond: cond_val,
            true_label: body_label,
            false_label: end_label,
        });

        self.start_block(body_label);
        self.lower_stmt(body);
        let continue_target = *self.func().continue_labels.last().unwrap();
        self.terminate(Terminator::Branch(continue_target));

        self.func_mut().break_labels.pop();
        self.func_mut().continue_labels.pop();
        self.start_block(end_label);
    }

    fn lower_for_stmt(
        &mut self,
        init: &Option<Box<ForInit>>,
        cond: &Option<Expr>,
        inc: &Option<Expr>,
        body: &Stmt,
    ) {
        // C99: for-init declarations have their own scope.
        let has_decl_init = init.as_ref().map_or(false, |i| matches!(i.as_ref(), ForInit::Declaration(_)));
        if has_decl_init {
            self.push_scope();
        }

        // Init
        if let Some(init) = init {
            match init.as_ref() {
                ForInit::Declaration(decl) => {
                    self.collect_enum_constants_scoped(&decl.type_spec);
                    self.lower_local_decl(decl);
                }
                ForInit::Expr(expr) => { self.lower_expr(expr); },
            }
        }

        let cond_label = self.fresh_label();
        let body_label = self.fresh_label();
        let inc_label = self.fresh_label();
        let end_label = self.fresh_label();

        self.func_mut().break_labels.push(end_label);
        self.func_mut().continue_labels.push(inc_label);

        self.terminate(Terminator::Branch(cond_label));

        // Condition
        self.start_block(cond_label);
        if let Some(cond) = cond {
            let cond_val = self.lower_condition_expr(cond);
            self.terminate(Terminator::CondBranch {
                cond: cond_val,
                true_label: body_label,
                false_label: end_label,
            });
        } else {
            self.terminate(Terminator::Branch(body_label));
        }

        // Body
        self.start_block(body_label);
        self.lower_stmt(body);
        self.terminate(Terminator::Branch(inc_label));

        // Increment
        self.start_block(inc_label);
        if let Some(inc) = inc {
            self.lower_expr(inc);
        }
        self.terminate(Terminator::Branch(cond_label));

        self.func_mut().break_labels.pop();
        self.func_mut().continue_labels.pop();
        self.start_block(end_label);

        if has_decl_init {
            self.pop_scope();
        }
    }

    fn lower_do_while_stmt(&mut self, body: &Stmt, cond: &Expr) {
        let body_label = self.fresh_label();
        let cond_label = self.fresh_label();
        let end_label = self.fresh_label();

        self.func_mut().break_labels.push(end_label);
        self.func_mut().continue_labels.push(cond_label);

        self.terminate(Terminator::Branch(body_label));

        self.start_block(body_label);
        self.lower_stmt(body);
        self.terminate(Terminator::Branch(cond_label));

        self.start_block(cond_label);
        let cond_val = self.lower_condition_expr(cond);
        self.terminate(Terminator::CondBranch {
            cond: cond_val,
            true_label: body_label,
            false_label: end_label,
        });

        self.func_mut().break_labels.pop();
        self.func_mut().continue_labels.pop();
        self.start_block(end_label);
    }

    fn lower_break_stmt(&mut self) {
        if let Some(&label) = self.func_mut().break_labels.last() {
            self.terminate(Terminator::Branch(label));
            let dead = self.fresh_label();
            self.start_block(dead);
        }
    }

    fn lower_continue_stmt(&mut self) {
        if let Some(&label) = self.func_mut().continue_labels.last() {
            self.terminate(Terminator::Branch(label));
            let dead = self.fresh_label();
            self.start_block(dead);
        }
    }

    fn lower_switch_stmt(&mut self, expr: &Expr, body: &Stmt) {
        // C99 6.8.4.2: Integer promotions are performed on the controlling expression.
        let raw_expr_ty = self.get_expr_type(expr);
        let switch_expr_ty = match raw_expr_ty {
            IrType::I8 | IrType::U8 | IrType::I16 | IrType::U16 => IrType::I32,
            _ => raw_expr_ty,
        };
        let val = self.lower_expr(expr);
        let val = if switch_expr_ty != raw_expr_ty {
            self.emit_implicit_cast(val, raw_expr_ty, switch_expr_ty)
        } else {
            val
        };

        // Store switch value in an alloca for dispatch chain reloading
        let switch_alloca = self.fresh_value();
        self.emit(Instruction::Alloca { dest: switch_alloca, ty: IrType::I64, size: 8, align: 0, volatile: false });
        self.emit(Instruction::Store { val, ptr: switch_alloca, ty: IrType::I64 , seg_override: AddressSpace::Default });

        let dispatch_label = self.fresh_label();
        let end_label = self.fresh_label();
        let body_label = self.fresh_label();

        // Push switch context
        self.func_mut().switch_stack.push(SwitchFrame {
            cases: Vec::new(),
            case_ranges: Vec::new(),
            default_label: None,
            expr_type: switch_expr_ty,
        });
        self.func_mut().break_labels.push(end_label);

        self.terminate(Terminator::Branch(dispatch_label));

        // Lower the body first; case/default stmts register their labels
        self.start_block(body_label);
        self.lower_stmt(body);
        self.terminate(Terminator::Branch(end_label));

        // Pop switch context and emit dispatch chain
        let switch_frame = self.func_mut().switch_stack.pop();
        self.func_mut().break_labels.pop();
        let cases = switch_frame.as_ref().map(|f| f.cases.clone()).unwrap_or_default();
        let case_ranges = switch_frame.as_ref().map(|f| f.case_ranges.clone()).unwrap_or_default();
        let default_label = switch_frame.as_ref().and_then(|f| f.default_label);

        let expr_type = switch_frame.as_ref().map(|f| f.expr_type).unwrap_or(IrType::I32);
        let fallback = default_label.unwrap_or(end_label);
        self.start_block(dispatch_label);
        self.emit_switch_dispatch(&val, switch_alloca, &cases, &case_ranges, fallback, expr_type);
        self.start_block(end_label);
    }

    /// Emit the dispatch chain for a switch statement. For compile-time constant
    /// switch values, directly jumps to the matching case. Otherwise emits a
    /// chain of comparison branches.
    fn emit_switch_dispatch(
        &mut self,
        val: &Operand,
        switch_alloca: Value,
        cases: &[(i64, BlockId)],
        case_ranges: &[(i64, i64, BlockId)],
        fallback: BlockId,
        switch_ty: IrType,
    ) {
        let total_checks = cases.len() + case_ranges.len();

        // Constant folding: if switch expression is a compile-time constant,
        // jump directly to the matching case (avoids dead dispatch branches
        // with type-mismatched inline asm).
        if let Operand::Const(c) = val {
            if let Some(switch_int) = self.const_to_i64(c) {
                let is_unsigned = switch_ty.is_unsigned();
                let target = cases.iter()
                    .find(|(cv, _)| *cv == switch_int)
                    .map(|(_, label)| *label)
                    .or_else(|| case_ranges.iter()
                        .find(|(low, high, _)| {
                            if is_unsigned {
                                (switch_int as u64) >= (*low as u64) && (switch_int as u64) <= (*high as u64)
                            } else {
                                switch_int >= *low && switch_int <= *high
                            }
                        })
                        .map(|(_, _, label)| *label))
                    .unwrap_or(fallback);
                self.terminate(Terminator::Branch(target));
                return;
            }
        }

        if total_checks == 0 {
            self.terminate(Terminator::Branch(fallback));
            return;
        }

        let mut check_idx = 0usize;
        // Emit equality checks for individual cases
        for (case_val, case_label) in cases.iter() {
            let loaded = self.fresh_value();
            self.emit(Instruction::Load { dest: loaded, ptr: switch_alloca, ty: IrType::I64 , seg_override: AddressSpace::Default });
            let cmp_result = self.emit_cmp_val(IrCmpOp::Eq, Operand::Value(loaded), Operand::Const(IrConst::I64(*case_val)), IrType::I64);

            check_idx += 1;
            let next_check = if check_idx < total_checks { self.fresh_label() } else { fallback };
            self.terminate(Terminator::CondBranch {
                cond: Operand::Value(cmp_result),
                true_label: *case_label,
                false_label: next_check,
            });
            if check_idx < total_checks {
                self.start_block(next_check);
            }
        }

        // Emit range checks: val >= low && val <= high
        // Use unsigned comparisons when the switch expression type is unsigned
        let is_unsigned = switch_ty.is_unsigned();
        let ge_op = if is_unsigned { IrCmpOp::Uge } else { IrCmpOp::Sge };
        let le_op = if is_unsigned { IrCmpOp::Ule } else { IrCmpOp::Sle };
        for (low, high, range_label) in case_ranges.iter() {
            let loaded = self.fresh_value();
            self.emit(Instruction::Load { dest: loaded, ptr: switch_alloca, ty: IrType::I64, seg_override: AddressSpace::Default });
            let ge_result = self.emit_cmp_val(ge_op, Operand::Value(loaded), Operand::Const(IrConst::I64(*low)), IrType::I64);
            let le_result = self.emit_cmp_val(le_op, Operand::Value(loaded), Operand::Const(IrConst::I64(*high)), IrType::I64);
            let and_result = self.fresh_value();
            self.emit(Instruction::BinOp {
                dest: and_result,
                op: crate::ir::ir::IrBinOp::And,
                lhs: Operand::Value(ge_result),
                rhs: Operand::Value(le_result),
                ty: IrType::I32,
            });

            check_idx += 1;
            let next_check = if check_idx < total_checks { self.fresh_label() } else { fallback };
            self.terminate(Terminator::CondBranch {
                cond: Operand::Value(and_result),
                true_label: *range_label,
                false_label: next_check,
            });
            if check_idx < total_checks {
                self.start_block(next_check);
            }
        }
    }

    /// Evaluate a case constant and truncate to the switch expression type.
    fn eval_case_constant(&mut self, expr: &Expr) -> i64 {
        let mut val = self.eval_const_expr(expr)
            .and_then(|c| self.const_to_i64(&c))
            .unwrap_or(0);
        if let Some(switch_ty) = self.func_mut().switch_stack.last().map(|f| &f.expr_type) {
            val = switch_ty.truncate_i64(val);
        }
        val
    }

    fn lower_case_stmt(&mut self, expr: &Expr, stmt: &Stmt) {
        let case_val = self.eval_case_constant(expr);
        let label = self.fresh_label();
        if let Some(frame) = self.func_mut().switch_stack.last_mut() {
            frame.cases.push((case_val, label));
        }
        // Fallthrough from previous case
        self.terminate(Terminator::Branch(label));
        self.start_block(label);
        self.lower_stmt(stmt);
    }

    fn lower_case_range_stmt(&mut self, low_expr: &Expr, high_expr: &Expr, stmt: &Stmt) {
        let low_val = self.eval_case_constant(low_expr);
        let high_val = self.eval_case_constant(high_expr);
        let label = self.fresh_label();
        if let Some(frame) = self.func_mut().switch_stack.last_mut() {
            frame.case_ranges.push((low_val, high_val, label));
        }
        self.terminate(Terminator::Branch(label));
        self.start_block(label);
        self.lower_stmt(stmt);
    }

    fn lower_default_stmt(&mut self, stmt: &Stmt) {
        let label = self.fresh_label();
        if let Some(frame) = self.func_mut().switch_stack.last_mut() {
            frame.default_label = Some(label);
        }
        self.terminate(Terminator::Branch(label));
        self.start_block(label);
        self.lower_stmt(stmt);
    }

    fn lower_goto_stmt(&mut self, label: &str) {
        // If the function has VLA declarations, restore the saved stack pointer before
        // jumping. This ensures VLA stack space is reclaimed on backward jumps (e.g.,
        // goto to a label before a VLA declaration in a loop).
        if let Some(save_val) = self.func().vla_stack_save {
            self.emit(Instruction::StackRestore { ptr: save_val });
        }
        let scoped_label = self.get_or_create_user_label(label);
        self.terminate(Terminator::Branch(scoped_label));
        let dead = self.fresh_label();
        self.start_block(dead);
    }

    fn lower_goto_indirect_stmt(&mut self, expr: &Expr) {
        // If the function has VLA declarations, restore the saved stack pointer before
        // indirect jumps too (computed gotos).
        if let Some(save_val) = self.func().vla_stack_save {
            self.emit(Instruction::StackRestore { ptr: save_val });
        }
        let target = self.lower_expr(expr);
        let possible_targets: Vec<BlockId> = self.func_mut().user_labels.values().copied().collect();
        self.terminate(Terminator::IndirectBranch { target, possible_targets });
        let dead = self.fresh_label();
        self.start_block(dead);
    }

    fn lower_label_stmt(&mut self, name: &str, stmt: &Stmt) {
        let label = self.get_or_create_user_label(name);
        self.terminate(Terminator::Branch(label));
        self.start_block(label);
        self.lower_stmt(stmt);
    }

    fn lower_inline_asm_stmt(
        &mut self,
        template: &str,
        outputs: &[AsmOperand],
        inputs: &[AsmOperand],
        clobbers: &[String],
        goto_labels: &[String],
    ) {
        let mut ir_outputs = Vec::new();
        let mut ir_inputs = Vec::new();
        let mut operand_types = Vec::new();
        let mut seg_overrides = Vec::new();

        // Process output operands and synthetic "+" inputs
        let mut plus_input_types = Vec::new();
        let mut plus_input_segs = Vec::new();
        for out in outputs {
            let mut constraint = out.constraint.clone();
            let name = out.name.clone();
            // Rewrite output constraint for register variables with __asm__("regname")
            if let Expr::Identifier(ref var_name, _) = out.expr {
                if let Some(asm_reg) = self.get_asm_register(var_name) {
                    let stripped = constraint.trim_start_matches(|c: char| c == '=' || c == '+' || c == '&');
                    if stripped.contains('r') || stripped == "g" {
                        let prefix: String = constraint.chars().take_while(|c| *c == '=' || *c == '+' || *c == '&').collect();
                        constraint = format!("{}{{{}}}", prefix, asm_reg);
                    }
                }
            }
            let out_ty = IrType::from_ctype(&self.expr_ctype(&out.expr));
            // Detect address space for memory operands (e.g., __seg_gs pointer dereferences)
            let out_seg = self.get_asm_operand_addr_space(&out.expr);
            let ptr = if let Some(lv) = self.lower_lvalue(&out.expr) {
                match lv {
                    LValue::Variable(v) | LValue::Address(v) => v,
                }
            } else if let Expr::Identifier(ref var_name, _) = out.expr {
                // Global register variables are pinned to specific hardware registers
                // via constraint rewriting above (e.g., "+r"  "+{rsp}"). They have no
                // backing storage, so lower_lvalue returns None. Create a temporary alloca
                // to preserve GCC operand numbering  without this, subsequent operand
                // references (e.g., %P4) would be off-by-one and unresolvable.
                // TODO: The alloca is a placeholder for operand numbering only.
                // The output value stored here is discarded; proper write-back to global
                // register variables is not yet implemented. For "+" constraints, the
                // Load below reads uninitialized memory, but the backend's constraint
                // rewriting to {regname} makes it read from the actual hardware register.
                if self.get_asm_register(var_name).is_some() {
                    let tmp = self.fresh_value();
                    self.emit(Instruction::Alloca {
                        dest: tmp, ty: out_ty, size: out_ty.size(),
                        align: out_ty.align(), volatile: false,
                    });
                    tmp
                } else {
                    continue;
                }
            } else {
                continue;
            };
            if constraint.contains('+') {
                let cur_val = self.fresh_value();
                self.emit(Instruction::Load { dest: cur_val, ptr, ty: out_ty, seg_override: out_seg });
                ir_inputs.push((constraint.replace('+', "").to_string(), Operand::Value(cur_val), name.clone()));
                plus_input_types.push(out_ty);
                plus_input_segs.push(out_seg);
            }
            ir_outputs.push((constraint, ptr, name));
            operand_types.push(out_ty);
            seg_overrides.push(out_seg);
        }
        for ty in plus_input_types {
            operand_types.push(ty);
        }
        for seg in plus_input_segs {
            seg_overrides.push(seg);
        }

        // Process input operands
        let mut input_symbols: Vec<Option<String>> = Vec::new();
        for inp in inputs {
            let mut constraint = inp.constraint.clone();
            let name = inp.name.clone();
            // Rewrite constraint for register variables with __asm__("regname"):
            // when the constraint allows "r", pin to the exact requested register.
            if let Expr::Identifier(ref var_name, _) = inp.expr {
                if let Some(asm_reg) = self.get_asm_register(var_name) {
                    let stripped = constraint.trim_start_matches(|c: char| c == '=' || c == '+' || c == '&');
                    if stripped.contains('r') || stripped == "g" {
                        constraint = format!("{{{}}}", asm_reg);
                    }
                }
            }
            let inp_ty = IrType::from_ctype(&self.expr_ctype(&inp.expr));
            let inp_seg = self.get_asm_operand_addr_space(&inp.expr);
            let mut sym_name: Option<String> = None;
            let val = if constraint_has_immediate_alt(&constraint) {
                if let Some(const_val) = self.eval_const_expr(&inp.expr) {
                    Operand::Const(const_val)
                } else {
                    sym_name = self.extract_symbol_name(&inp.expr);
                    self.lower_expr(&inp.expr)
                }
            } else if constraint_is_memory_only(&constraint) {
                if let Some(lv) = self.lower_lvalue(&inp.expr) {
                    let ptr = match lv {
                        LValue::Variable(v) | LValue::Address(v) => v,
                    };
                    Operand::Value(ptr)
                } else {
                    self.lower_expr(&inp.expr)
                }
            } else {
                self.lower_expr(&inp.expr)
            };
            ir_inputs.push((constraint, val, name));
            operand_types.push(inp_ty);
            input_symbols.push(sym_name);
            seg_overrides.push(inp_seg);
        }

        // Resolve goto labels
        let ir_goto_labels: Vec<(String, BlockId)> = goto_labels.iter().map(|name| {
            let block = self.get_or_create_user_label(name);
            (name.clone(), block)
        }).collect();

        self.emit(Instruction::InlineAsm {
            template: template.to_string(),
            outputs: ir_outputs,
            inputs: ir_inputs,
            clobbers: clobbers.to_vec(),
            operand_types,
            goto_labels: ir_goto_labels,
            input_symbols,
            seg_overrides,
        });
    }

    /// Extract a global symbol name from an expression, for use with inline asm
    /// `"i"` constraint operands and `%P`/`%c`/`%a` modifiers. Handles:
    /// - `func_name` (bare function identifier that is a known function or global)
    /// - `&var_name` (address-of global variable)
    /// - Casts of the above (e.g., `(void *)func_name`)
    ///
    /// Returns `None` for local variables/parameters, since those are not valid
    /// assembly symbols and would produce invalid assembly if emitted literally.
    fn extract_symbol_name(&self, expr: &Expr) -> Option<String> {
        match expr {
            Expr::Identifier(name, _) => {
                // Only return the name if it is a global symbol or known function,
                // NOT a local variable or function parameter.
                if self.is_global_or_function(name) {
                    Some(name.clone())
                } else {
                    None
                }
            }
            Expr::AddressOf(inner, _) => {
                if let Expr::Identifier(name, _) = inner.as_ref() {
                    if self.is_global_or_function(name) {
                        Some(name.clone())
                    } else {
                        None
                    }
                } else {
                    None
                }
            }
            Expr::Cast(_, inner, _) => self.extract_symbol_name(inner),
            _ => None,
        }
    }

    /// Check whether a name refers to a global variable, known function, or
    /// enum constant (i.e., something that is a valid assembly-level symbol or
    /// compile-time constant), as opposed to a local variable or parameter.
    fn is_global_or_function(&self, name: &str) -> bool {
        // Check if it's a local variable/parameter first  if so, it's NOT global
        if let Some(ref fs) = self.func_state {
            if fs.locals.contains_key(name) {
                return false;
            }
        }
        // It's a global if it's in the globals map, known functions, or enum constants
        self.globals.contains_key(name)
            || self.known_functions.contains(name)
            || self.types.enum_constants.contains_key(name)
    }

    /// Look up the asm register name for a variable declared with
    /// `register <type> <name> __asm__("regname")`.
    /// Checks local variables first, then global register variables.
    fn get_asm_register(&self, name: &str) -> Option<String> {
        // Check locals first
        if let Some(reg) = self.func_state.as_ref()
            .and_then(|fs| fs.locals.get(name))
            .and_then(|info| info.asm_register.clone())
        {
            return Some(reg);
        }
        // Check globals (for global register variables like `current_stack_pointer`)
        self.globals.get(name)
            .and_then(|info| info.asm_register.clone())
    }

    /// Detect address space for an inline asm operand expression.
    /// For expressions like `*(typeof(var) __seg_gs *)(uintptr_t)&var`,
    /// returns the address space from the pointer type in the deref.
    fn get_asm_operand_addr_space(&self, expr: &Expr) -> AddressSpace {
        match expr {
            Expr::Deref(inner, _) => self.get_addr_space_of_ptr_expr(inner),
            _ => AddressSpace::Default,
        }
    }

    /// Compute the runtime sizeof for a VLA local variable.
    /// Returns Some(Value) if any array dimension is a non-constant expression.
    /// The Value holds the total byte size (product of all dimensions * element_size).
    /// Returns None if all dimensions are compile-time constants.
    pub(super) fn compute_vla_runtime_size(
        &mut self,
        type_spec: &TypeSpecifier,
        derived: &[DerivedDeclarator],
    ) -> Option<Value> {
        // Collect array dimensions from derived declarators
        let array_dims: Vec<&Option<Box<Expr>>> = derived.iter().filter_map(|d| {
            if let DerivedDeclarator::Array(size) = d {
                Some(size)
            } else {
                None
            }
        }).collect();

        if array_dims.is_empty() {
            // Check if the type_spec itself is an Array (typedef'd VLA)
            return self.compute_vla_size_from_type_spec(type_spec);
        }

        // Check if any dimension is non-constant
        let mut has_vla = false;
        for dim in &array_dims {
            if let Some(expr) = dim {
                if self.expr_as_array_size(expr).is_none() {
                    has_vla = true;
                    break;
                }
            }
        }

        if !has_vla {
            return None; // All dimensions are compile-time constants
        }

        // Compute element size (the base type size)
        let base_elem_size = self.sizeof_type(type_spec);

        // Build runtime product: dim0 * dim1 * ... * base_elem_size
        let mut result: Option<Value> = None;
        let mut const_product: usize = base_elem_size;

        for dim in &array_dims {
            if let Some(expr) = dim {
                if let Some(const_val) = self.expr_as_array_size(expr) {
                    // Constant dimension - accumulate
                    const_product *= const_val as usize;
                } else {
                    // Runtime dimension - emit multiplication
                    let dim_val = self.lower_expr(expr);
                    let dim_value = self.operand_to_value(dim_val);

                    result = if let Some(prev) = result {
                        let mul = self.emit_binop_val(IrBinOp::Mul, Operand::Value(prev), Operand::Value(dim_value), IrType::I64);
                        Some(mul)
                    } else {
                        // First runtime dim: multiply by accumulated constants
                        if const_product > 1 {
                            let mul = self.emit_binop_val(IrBinOp::Mul, Operand::Value(dim_value), Operand::Const(IrConst::I64(const_product as i64)), IrType::I64);
                            const_product = 1;
                            Some(mul)
                        } else {
                            Some(dim_value)
                        }
                    };
                }
            }
        }

        // If we have remaining constant factors, multiply them in
        if let Some(prev) = result {
            if const_product > 1 {
                let mul = self.emit_binop_val(IrBinOp::Mul, Operand::Value(prev), Operand::Const(IrConst::I64(const_product as i64)), IrType::I64);
                return Some(mul);
            }
            return Some(prev);
        }

        None
    }

    /// Compute VLA strides for a local VLA variable declaration.
    ///
    /// For `double a[n][m]`, we need strides:
    ///   stride[0] = m * sizeof(double)   (stride for first dimension = row stride)
    ///   stride[1] = sizeof(double)       (stride for second dimension = element stride)
    ///
    /// The strides array has one entry per array dimension. Each entry is
    /// `Some(Value)` if the stride requires a runtime computation, or `None`
    /// if it's a compile-time constant (handled by the fallback path).
    ///
    /// We process dimensions from innermost to outermost, accumulating the
    /// product of inner dimensions * base element size.
    pub(super) fn compute_vla_local_strides(
        &mut self,
        type_spec: &TypeSpecifier,
        derived: &[DerivedDeclarator],
    ) -> Vec<Option<Value>> {
        // Collect array dimensions from derived declarators
        let array_dims: Vec<&Option<Box<Expr>>> = derived.iter().filter_map(|d| {
            if let DerivedDeclarator::Array(size) = d {
                Some(size)
            } else {
                None
            }
        }).collect();

        if array_dims.len() < 2 {
            // For 1D VLAs, no stride info needed (the element size is known at compile time)
            return vec![];
        }

        // Check if any dimension is a VLA
        let mut has_vla = false;
        for dim in &array_dims {
            if let Some(expr) = dim {
                if self.expr_as_array_size(expr).is_none() {
                    has_vla = true;
                    break;
                }
            }
        }
        if !has_vla {
            return vec![];
        }

        let base_elem_size = self.sizeof_type(type_spec);
        let num_dims = array_dims.len();
        let num_strides = num_dims + 1; // +1 for base element size level
        let mut vla_strides: Vec<Option<Value>> = vec![None; num_strides];

        // Process dimensions from innermost (last) to outermost (first).
        // Each stride[i] = product of all inner dimension sizes * base_elem_size.
        // stride[num_dims-1] is always base_elem_size (the innermost element stride).
        // stride[i] = array_dims[i+1] * stride[i+1] for i < num_dims-1.
        let mut current_stride: Option<Value> = None;
        let mut current_const_stride: usize = base_elem_size;

        for i in (0..num_dims).rev() {
            if i == num_dims - 1 {
                // Innermost dimension: stride is base_elem_size (compile-time constant)
                // No need to set vla_strides[i] since the fallback handles constants.
                // But we need to track it for computing outer strides.
                continue;
            }

            // The stride for dimension i = dimension_size[i+1] * stride[i+1]
            // We need to compute this from the (i+1)th dimension.
            let inner_dim = &array_dims[i + 1];
            if let Some(expr) = inner_dim {
                if let Some(const_val) = self.expr_as_array_size(expr) {
                    // Inner dimension is a compile-time constant
                    current_const_stride *= const_val as usize;
                    if current_stride.is_some() {
                        // Previous stride was runtime, multiply by constant
                        let stride_val = self.emit_binop_val(
                            IrBinOp::Mul,
                            Operand::Value(current_stride.unwrap()),
                            Operand::Const(IrConst::I64(const_val as i64)),
                            IrType::I64,
                        );
                        current_stride = Some(stride_val);
                        vla_strides[i] = Some(stride_val);
                    }
                    // else: purely const, fallback handles it
                } else {
                    // Inner dimension is a runtime VLA dimension
                    let dim_val = self.lower_expr(expr);
                    let dim_value = self.operand_to_value(dim_val);

                    let stride_val = if let Some(prev) = current_stride {
                        self.emit_binop_val(
                            IrBinOp::Mul,
                            Operand::Value(dim_value),
                            Operand::Value(prev),
                            IrType::I64,
                        )
                    } else {
                        // First runtime dimension: multiply by accumulated const
                        if current_const_stride > 1 {
                            self.emit_binop_val(
                                IrBinOp::Mul,
                                Operand::Value(dim_value),
                                Operand::Const(IrConst::I64(current_const_stride as i64)),
                                IrType::I64,
                            )
                        } else {
                            dim_value
                        }
                    };
                    current_stride = Some(stride_val);
                    current_const_stride = 0;
                    vla_strides[i] = Some(stride_val);
                }
            }
        }

        vla_strides
    }

    /// Compute VLA size from a typedef'd array type (e.g., typedef char buf[n]).
    fn compute_vla_size_from_type_spec(&mut self, type_spec: &TypeSpecifier) -> Option<Value> {
        let resolved = self.resolve_type_spec(type_spec).clone();
        match &resolved {
            TypeSpecifier::Array(elem, Some(size_expr)) => {
                if self.expr_as_array_size(size_expr).is_some() {
                    return None; // Constant size
                }
                // Clone what we need before mutable borrow
                let size_expr_clone = size_expr.clone();
                let elem_size = self.sizeof_type(elem);
                // Runtime size expression
                let dim_val = self.lower_expr(&size_expr_clone);
                let dim_value = self.operand_to_value(dim_val);
                if elem_size > 1 {
                    let mul = self.emit_binop_val(IrBinOp::Mul, Operand::Value(dim_value), Operand::Const(IrConst::I64(elem_size as i64)), IrType::I64);
                    Some(mul)
                } else {
                    Some(dim_value)
                }
            }
            _ => None,
        }
    }
}

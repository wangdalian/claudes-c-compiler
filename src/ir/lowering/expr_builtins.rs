//! Builtin function call lowering: __builtin_* intrinsics and FP classification.
//!
//! Extracted from expr.rs to keep the expression lowering manageable.
//! This module handles:
//! - `try_lower_builtin_call`: main dispatch for all __builtin_* functions
//! - Intrinsic helpers (clz, ctz, bswap, popcount, parity)
//! - FP classification builtins (fpclassify, isnan, isinf, isfinite, isnormal, signbit)
//! - `creal_return_type`: return type for creal/cimag family
//! - `builtin_return_type` lookup (defined on Lowerer in lowering.rs)

use crate::frontend::parser::ast::*;
use crate::frontend::sema::builtins::{self, BuiltinKind, BuiltinIntrinsic};
use crate::ir::ir::*;
use crate::common::types::{AddressSpace, IrType, CType};
use super::lowering::Lowerer;

impl Lowerer {
    /// Determine the return type for creal/crealf/creall/cimag/cimagf/cimagl based on function name.
    pub(super) fn creal_return_type(name: &str) -> IrType {
        match name {
            "crealf" | "__builtin_crealf" | "cimagf" | "__builtin_cimagf" => IrType::F32,
            "creall" | "__builtin_creall" | "cimagl" | "__builtin_cimagl" => IrType::F128,
            _ => IrType::F64, // creal, cimag, __builtin_creal, __builtin_cimag
        }
    }

    /// Try to lower a __builtin_* call. Returns Some(result) if handled.
    pub(super) fn try_lower_builtin_call(&mut self, name: &str, args: &[Expr]) -> Option<Operand> {
        // Handle __builtin_choose_expr(const_expr, expr1, expr2)
        // This is a compile-time selection: if const_expr is nonzero, returns expr1, else expr2.
        if name == "__builtin_choose_expr" {
            if args.len() >= 3 {
                let condition = self.eval_const_expr(&args[0]);
                let is_nonzero = match condition {
                    Some(IrConst::I64(v)) => v != 0,
                    Some(IrConst::I32(v)) => v != 0,
                    Some(IrConst::I128(v)) => v != 0,
                    Some(IrConst::F64(v)) => v != 0.0,
                    Some(IrConst::F32(v)) => v != 0.0,
                    _ => {
                        // Try lowering to get a constant
                        let cond_val = self.lower_expr(&args[0]);
                        match cond_val {
                            Operand::Const(IrConst::I64(v)) => v != 0,
                            Operand::Const(IrConst::I32(v)) => v != 0,
                            _ => true, // default to first expr if indeterminate
                        }
                    }
                };
                return Some(if is_nonzero {
                    self.lower_expr(&args[1])
                } else {
                    self.lower_expr(&args[2])
                });
            }
            return Some(Operand::Const(IrConst::I64(0)));
        }

        // Handle alloca specially - it needs dynamic stack allocation
        match name {
            "__builtin_alloca" | "__builtin_alloca_with_align" => {
                if let Some(size_expr) = args.first() {
                    let size_operand = self.lower_expr(size_expr);
                    let align = if name == "__builtin_alloca_with_align" && args.len() >= 2 {
                        // align is in bits
                        if let Expr::IntLiteral(bits, _) = &args[1] {
                            (*bits as usize) / 8
                        } else {
                            16
                        }
                    } else {
                        16 // default alignment
                    };
                    let dest = self.fresh_value();
                    self.emit(Instruction::DynAlloca { dest, size: size_operand, align });
                    return Some(Operand::Value(dest));
                }
                return Some(Operand::Const(IrConst::I64(0)));
            }
            _ => {}
        }
        // Handle va_start/va_end/va_copy specially.
        // These builtins need a pointer to the va_list struct.
        // Use lower_va_list_pointer which correctly handles both local va_list
        // variables (array type, needs address-of) and va_list parameters
        // (pointer type after array-to-pointer decay, needs value load).
        match name {
            "__builtin_va_start" => {
                if let Some(ap_expr) = args.first() {
                    let ap_ptr_op = self.lower_va_list_pointer(ap_expr);
                    let ap_ptr = self.operand_to_value(ap_ptr_op);
                    self.emit(Instruction::VaStart { va_list_ptr: ap_ptr });
                }
                return Some(Operand::Const(IrConst::I64(0)));
            }
            "__builtin_va_end" => {
                if let Some(ap_expr) = args.first() {
                    let ap_ptr_op = self.lower_va_list_pointer(ap_expr);
                    let ap_ptr = self.operand_to_value(ap_ptr_op);
                    self.emit(Instruction::VaEnd { va_list_ptr: ap_ptr });
                }
                return Some(Operand::Const(IrConst::I64(0)));
            }
            "__builtin_va_copy" => {
                if args.len() >= 2 {
                    let dest_op = self.lower_va_list_pointer(&args[0]);
                    let src_op = self.lower_va_list_pointer(&args[1]);
                    let dest_ptr = self.operand_to_value(dest_op);
                    let src_ptr = self.operand_to_value(src_op);
                    self.emit(Instruction::VaCopy { dest_ptr, src_ptr });
                }
                return Some(Operand::Const(IrConst::I64(0)));
            }
            _ => {}
        }

        // __builtin_expect(exp, c) - branch prediction hint.
        // Returns exp unchanged, but must evaluate c for side effects.
        // __builtin_expect_with_probability(exp, c, prob) - same with probability.
        if name == "__builtin_expect" || name == "__builtin_expect_with_probability" {
            let result = if let Some(first) = args.first() {
                self.lower_expr(first)
            } else {
                Operand::Const(IrConst::I64(0))
            };
            // Evaluate remaining arguments for their side effects
            for arg in args.iter().skip(1) {
                self.lower_expr(arg);
            }
            return Some(result);
        }

        // __builtin_prefetch(addr, [rw], [locality]) - no-op performance hint
        if name == "__builtin_prefetch" {
            for arg in args {
                self.lower_expr(arg);
            }
            return Some(Operand::Const(IrConst::I64(0)));
        }

        // __builtin_unreachable() - marks code path as unreachable. Emit a trap instruction
        // (ud2 on x86, brk #0 on ARM, ebreak on RISC-V) via Terminator::Unreachable.
        // This must NOT generate a call to abort() because abort() may not exist
        // (e.g., in kernel code).
        if name == "__builtin_unreachable" {
            self.terminate(Terminator::Unreachable);
            // Start a new (unreachable) block so subsequent code can still be lowered
            let dead_label = self.fresh_label();
            self.start_block(dead_label);
            return Some(Operand::Const(IrConst::I64(0)));
        }

        // __builtin_trap() - generates a trap instruction to intentionally crash.
        // Like unreachable, this must emit the hardware trap directly, not call abort().
        if name == "__builtin_trap" {
            self.terminate(Terminator::Unreachable);
            let dead_label = self.fresh_label();
            self.start_block(dead_label);
            return Some(Operand::Const(IrConst::I64(0)));
        }

        // Handle atomic builtins (delegated to expr_atomics.rs)
        if let Some(result) = self.try_lower_atomic_builtin(name, args) {
            return Some(result);
        }

        let builtin_info = builtins::resolve_builtin(name)?;
        match &builtin_info.kind {
            BuiltinKind::LibcAlias(libc_name) => {
                let arg_types: Vec<IrType> = args.iter().map(|a| self.get_expr_type(a)).collect();
                let arg_vals: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let dest = self.fresh_value();
                let libc_sig = self.func_meta.sigs.get(libc_name.as_str());
                let variadic = libc_sig.map_or(false, |s| s.is_variadic);
                let n_fixed = if variadic {
                    libc_sig.map(|s| s.param_types.len()).unwrap_or(arg_vals.len())
                } else { arg_vals.len() };
                let return_type = Self::builtin_return_type(name)
                    .or_else(|| libc_sig.map(|s| s.return_type))
                    .unwrap_or(IrType::I64);
                let struct_arg_sizes = vec![None; arg_vals.len()];
                self.emit(Instruction::Call {
                    dest: Some(dest), func: libc_name.clone(),
                    args: arg_vals, arg_types, return_type, is_variadic: variadic, num_fixed_args: n_fixed,
                    struct_arg_sizes,
                });
                Some(Operand::Value(dest))
            }
            BuiltinKind::Identity => {
                Some(args.first().map_or(Operand::Const(IrConst::I64(0)), |a| self.lower_expr(a)))
            }
            BuiltinKind::ConstantI64(val) => Some(Operand::Const(IrConst::I64(*val))),
            BuiltinKind::ConstantF64(val) => {
                let is_float_variant = name == "__builtin_inff"
                    || name == "__builtin_huge_valf"
                    || name == "__builtin_nanf";
                let is_long_double_variant = name == "__builtin_infl"
                    || name == "__builtin_huge_vall"
                    || name == "__builtin_nanl";
                if is_float_variant {
                    Some(Operand::Const(IrConst::F32(*val as f32)))
                } else if is_long_double_variant {
                    Some(Operand::Const(IrConst::long_double(*val)))
                } else {
                    Some(Operand::Const(IrConst::F64(*val)))
                }
            }
            BuiltinKind::Intrinsic(intrinsic) => {
                self.lower_builtin_intrinsic(intrinsic, name, args)
            }
        }
    }

    /// Lower a builtin intrinsic (the BuiltinKind::Intrinsic arm of try_lower_builtin_call).
    fn lower_builtin_intrinsic(&mut self, intrinsic: &BuiltinIntrinsic, name: &str, args: &[Expr]) -> Option<Operand> {
        match intrinsic {
            BuiltinIntrinsic::FpCompare => {
                if args.len() >= 2 {
                    let lhs_ty = self.get_expr_type(&args[0]);
                    let rhs_ty = self.get_expr_type(&args[1]);
                    // Promote to the widest floating-point type among the two operands.
                    let cmp_ty = if lhs_ty == IrType::F128 || rhs_ty == IrType::F128 {
                        IrType::F128
                    } else if lhs_ty == IrType::F64 || rhs_ty == IrType::F64 {
                        IrType::F64
                    } else if lhs_ty == IrType::F32 || rhs_ty == IrType::F32 {
                        IrType::F32
                    } else {
                        IrType::F64
                    };
                    let mut lhs = self.lower_expr(&args[0]);
                    let mut rhs = self.lower_expr(&args[1]);
                    if lhs_ty != cmp_ty {
                        let conv = self.emit_cast_val(lhs, lhs_ty, cmp_ty);
                        lhs = Operand::Value(conv);
                    }
                    if rhs_ty != cmp_ty {
                        let conv = self.emit_cast_val(rhs, rhs_ty, cmp_ty);
                        rhs = Operand::Value(conv);
                    }

                    // __builtin_isunordered: returns 1 if either operand is NaN.
                    // Emit (a != a) | (b != b) since NaN is the only value where x != x.
                    if name == "__builtin_isunordered" {
                        let lhs_nan = self.emit_cmp_val(IrCmpOp::Ne, lhs.clone(), lhs.clone(), cmp_ty);
                        let rhs_nan = self.emit_cmp_val(IrCmpOp::Ne, rhs.clone(), rhs.clone(), cmp_ty);
                        let dest = self.emit_binop_val(IrBinOp::Or, Operand::Value(lhs_nan), Operand::Value(rhs_nan), IrType::I32);
                        return Some(Operand::Value(dest));
                    }

                    // __builtin_islessgreater: returns 1 if a < b or a > b (NOT for NaN).
                    // This differs from (a != b) because NaN != NaN is true, but
                    // islessgreater(NaN, x) must be false.
                    // Emit (a < b) | (a > b).
                    if name == "__builtin_islessgreater" {
                        let lt = self.emit_cmp_val(IrCmpOp::Slt, lhs.clone(), rhs.clone(), cmp_ty);
                        let gt = self.emit_cmp_val(IrCmpOp::Sgt, lhs, rhs, cmp_ty);
                        let dest = self.emit_binop_val(IrBinOp::Or, Operand::Value(lt), Operand::Value(gt), IrType::I32);
                        return Some(Operand::Value(dest));
                    }

                    let cmp_op = match name {
                        "__builtin_isgreater" => IrCmpOp::Sgt,
                        "__builtin_isgreaterequal" => IrCmpOp::Sge,
                        "__builtin_isless" => IrCmpOp::Slt,
                        "__builtin_islessequal" => IrCmpOp::Sle,
                        _ => IrCmpOp::Eq,
                    };
                    let dest = self.emit_cmp_val(cmp_op, lhs, rhs, cmp_ty);
                    return Some(Operand::Value(dest));
                }
                Some(Operand::Const(IrConst::I64(0)))
            }
            BuiltinIntrinsic::AddOverflow => self.lower_overflow_builtin(name, args, IrBinOp::Add),
            BuiltinIntrinsic::SubOverflow => self.lower_overflow_builtin(name, args, IrBinOp::Sub),
            BuiltinIntrinsic::MulOverflow => self.lower_overflow_builtin(name, args, IrBinOp::Mul),
            BuiltinIntrinsic::Clz => self.lower_unary_intrinsic(name, args, IrUnaryOp::Clz),
            BuiltinIntrinsic::Ctz => self.lower_unary_intrinsic(name, args, IrUnaryOp::Ctz),
            BuiltinIntrinsic::Ffs => self.lower_ffs_intrinsic(name, args),
            BuiltinIntrinsic::Clrsb => self.lower_clrsb_intrinsic(name, args),
            BuiltinIntrinsic::Bswap => self.lower_bswap_intrinsic(name, args),
            BuiltinIntrinsic::Popcount => self.lower_unary_intrinsic(name, args, IrUnaryOp::Popcount),
            BuiltinIntrinsic::Parity => self.lower_parity_intrinsic(name, args),
            BuiltinIntrinsic::ComplexReal => {
                if !args.is_empty() {
                    let arg_ctype = self.expr_ctype(&args[0]);
                    let target_ty = Self::creal_return_type(name);
                    if arg_ctype.is_complex() {
                        let val = self.lower_complex_real_part(&args[0]);
                        // Cast from component type to target return type
                        // e.g., creal(float_complex) extracts F32 but must return F64
                        let comp_ty = Self::complex_component_ir_type(&arg_ctype);
                        if comp_ty != target_ty {
                            Some(self.emit_implicit_cast(val, comp_ty, target_ty))
                        } else {
                            Some(val)
                        }
                    } else {
                        let val = self.lower_expr(&args[0]);
                        let val_ty = self.get_expr_type(&args[0]);
                        Some(self.emit_implicit_cast(val, val_ty, target_ty))
                    }
                } else {
                    Some(Operand::Const(IrConst::F64(0.0)))
                }
            }
            BuiltinIntrinsic::ComplexImag => {
                if !args.is_empty() {
                    let arg_ctype = self.expr_ctype(&args[0]);
                    let target_ty = Self::creal_return_type(name);
                    if arg_ctype.is_complex() {
                        let val = self.lower_complex_imag_part(&args[0]);
                        // Cast from component type to target return type
                        let comp_ty = Self::complex_component_ir_type(&arg_ctype);
                        if comp_ty != target_ty {
                            Some(self.emit_implicit_cast(val, comp_ty, target_ty))
                        } else {
                            Some(val)
                        }
                    } else {
                        Some(match target_ty {
                            IrType::F32 => Operand::Const(IrConst::F32(0.0)),
                            IrType::F128 => Operand::Const(IrConst::long_double(0.0)),
                            _ => Operand::Const(IrConst::F64(0.0)),
                        })
                    }
                } else {
                    Some(Operand::Const(IrConst::F64(0.0)))
                }
            }
            BuiltinIntrinsic::ComplexConj => {
                if !args.is_empty() {
                    Some(self.lower_complex_conj(&args[0]))
                } else {
                    Some(Operand::Const(IrConst::F64(0.0)))
                }
            }
            BuiltinIntrinsic::Fence => {
                Some(Operand::Const(IrConst::I64(0)))
            }
            BuiltinIntrinsic::FpClassify => self.lower_builtin_fpclassify(args),
            BuiltinIntrinsic::IsNan => self.lower_builtin_isnan(args),
            BuiltinIntrinsic::IsInf => self.lower_builtin_isinf(args),
            BuiltinIntrinsic::IsFinite => self.lower_builtin_isfinite(args),
            BuiltinIntrinsic::IsNormal => self.lower_builtin_isnormal(args),
            BuiltinIntrinsic::SignBit => self.lower_builtin_signbit(args),
            BuiltinIntrinsic::IsInfSign => self.lower_builtin_isinf_sign(args),
            BuiltinIntrinsic::Alloca => {
                // Handled earlier in try_lower_builtin_call - should not reach here
                Some(Operand::Const(IrConst::I64(0)))
            }
            BuiltinIntrinsic::ComplexConstruct => {
                if args.len() >= 2 {
                    let real_val = self.lower_expr(&args[0]);
                    let imag_val = self.lower_expr(&args[1]);
                    let arg_ty = self.get_expr_type(&args[0]);
                    let (comp_ty, complex_size, comp_size) = if arg_ty == IrType::F32 {
                        (IrType::F32, 8usize, 4usize)
                    } else {
                        (IrType::F64, 16usize, 8usize)
                    };
                    let alloca = self.fresh_value();
                    self.emit(Instruction::Alloca { dest: alloca, ty: IrType::Ptr, size: complex_size, align: 0, volatile: false });
                    self.emit(Instruction::Store { val: real_val, ptr: alloca, ty: comp_ty , seg_override: AddressSpace::Default });
                    let imag_ptr = self.fresh_value();
                    self.emit(Instruction::GetElementPtr {
                        dest: imag_ptr, base: alloca,
                        offset: Operand::Const(IrConst::I64(comp_size as i64)),
                        ty: IrType::I8,
                    });
                    self.emit(Instruction::Store { val: imag_val, ptr: imag_ptr, ty: comp_ty , seg_override: AddressSpace::Default });
                    Some(Operand::Value(alloca))
                } else {
                    Some(Operand::Const(IrConst::I64(0)))
                }
            }
            BuiltinIntrinsic::VaStart | BuiltinIntrinsic::VaEnd | BuiltinIntrinsic::VaCopy => {
                unreachable!("va builtins handled earlier by name match")
            }
            // __builtin_constant_p(expr) -> 1 if expr is a compile-time constant, 0 otherwise
            BuiltinIntrinsic::ConstantP => {
                let result = if let Some(arg) = args.first() {
                    if self.eval_const_expr(arg).is_some() { 1i64 } else { 0i64 }
                } else {
                    0i64
                };
                Some(Operand::Const(IrConst::I64(result)))
            }
            // __builtin_object_size(ptr, type) -> compile-time object size, or unknown
            // For types 0 and 1: return (size_t)-1 when size is unknown
            // For types 2 and 3: return 0 when size is unknown
            // We conservatively return "unknown" since we don't do points-to analysis.
            BuiltinIntrinsic::ObjectSize => {
                // Evaluate args for side effects
                for arg in args {
                    self.lower_expr(arg);
                }
                let obj_type = if args.len() >= 2 {
                    match self.eval_const_expr(&args[1]) {
                        Some(IrConst::I64(v)) => v,
                        Some(IrConst::I32(v)) => v as i64,
                        _ => 0,
                    }
                } else {
                    0
                };
                // Types 0 and 1: maximum estimate -> -1 (unknown)
                // Types 2 and 3: minimum estimate -> 0 (unknown)
                let result = if obj_type == 2 || obj_type == 3 { 0i64 } else { -1i64 };
                Some(Operand::Const(IrConst::I64(result)))
            }
            // __builtin_classify_type(expr) -> GCC type class integer
            BuiltinIntrinsic::ClassifyType => {
                let result = if let Some(arg) = args.first() {
                    // Special case: bare function identifiers decay to pointer
                    // type (class 5). expr_ctype may return CType::Int as
                    // fallback since functions aren't stored as variables.
                    if let Expr::Identifier(fname, _) = arg {
                        if self.known_functions.contains(fname.as_str()) {
                            5i64 // pointer_type_class (function decays to pointer)
                        } else {
                            let ctype = self.expr_ctype(arg);
                            classify_ctype(&ctype)
                        }
                    } else {
                        let ctype = self.expr_ctype(arg);
                        classify_ctype(&ctype)
                    }
                } else {
                    0i64 // no_type_class
                };
                Some(Operand::Const(IrConst::I64(result)))
            }
            // Nop intrinsic - just evaluate args for side effects and return 0
            BuiltinIntrinsic::Nop => {
                for arg in args {
                    self.lower_expr(arg);
                }
                Some(Operand::Const(IrConst::I64(0)))
            }
            // X86 SSE fence/barrier operations (no dest, no meaningful return)
            BuiltinIntrinsic::X86Lfence => {
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Lfence, dest_ptr: None, args: vec![] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            BuiltinIntrinsic::X86Mfence => {
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Mfence, dest_ptr: None, args: vec![] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            BuiltinIntrinsic::X86Sfence => {
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Sfence, dest_ptr: None, args: vec![] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            BuiltinIntrinsic::X86Pause => {
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Pause, dest_ptr: None, args: vec![] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // clflush(ptr)
            BuiltinIntrinsic::X86Clflush => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Clflush, dest_ptr: None, args: arg_ops });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // Non-temporal store: movnti(ptr, val) - 32-bit
            BuiltinIntrinsic::X86Movnti => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let ptr_val = self.operand_to_value(arg_ops[0].clone());
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Movnti, dest_ptr: Some(ptr_val), args: vec![arg_ops[1].clone()] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // Non-temporal store: movnti64(ptr, val) - 64-bit
            BuiltinIntrinsic::X86Movnti64 => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let ptr_val = self.operand_to_value(arg_ops[0].clone());
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Movnti64, dest_ptr: Some(ptr_val), args: vec![arg_ops[1].clone()] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // Non-temporal store 128-bit: movntdq(ptr, src_ptr)
            BuiltinIntrinsic::X86Movntdq => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let ptr_val = self.operand_to_value(arg_ops[0].clone());
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Movntdq, dest_ptr: Some(ptr_val), args: vec![arg_ops[1].clone()] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // Non-temporal store 128-bit double: movntpd(ptr, src_ptr)
            BuiltinIntrinsic::X86Movntpd => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let ptr_val = self.operand_to_value(arg_ops[0].clone());
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Movntpd, dest_ptr: Some(ptr_val), args: vec![arg_ops[1].clone()] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // 128-bit operations that return __m128i (16-byte struct via pointer)
            // These allocate a 16-byte stack slot for the result and return a pointer to it
            BuiltinIntrinsic::X86Loaddqu
            | BuiltinIntrinsic::X86Pcmpeqb128
            | BuiltinIntrinsic::X86Pcmpeqd128
            | BuiltinIntrinsic::X86Psubusb128
            | BuiltinIntrinsic::X86Por128
            | BuiltinIntrinsic::X86Pand128
            | BuiltinIntrinsic::X86Pxor128
            | BuiltinIntrinsic::X86Set1Epi8
            | BuiltinIntrinsic::X86Set1Epi32 => {
                let sse_op = match intrinsic {
                    BuiltinIntrinsic::X86Loaddqu => IntrinsicOp::Loaddqu,
                    BuiltinIntrinsic::X86Pcmpeqb128 => IntrinsicOp::Pcmpeqb128,
                    BuiltinIntrinsic::X86Pcmpeqd128 => IntrinsicOp::Pcmpeqd128,
                    BuiltinIntrinsic::X86Psubusb128 => IntrinsicOp::Psubusb128,
                    BuiltinIntrinsic::X86Por128 => IntrinsicOp::Por128,
                    BuiltinIntrinsic::X86Pand128 => IntrinsicOp::Pand128,
                    BuiltinIntrinsic::X86Pxor128 => IntrinsicOp::Pxor128,
                    BuiltinIntrinsic::X86Set1Epi8 => IntrinsicOp::SetEpi8,
                    BuiltinIntrinsic::X86Set1Epi32 => IntrinsicOp::SetEpi32,
                    _ => unreachable!(),
                };
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                // Allocate 16-byte stack slot for result
                let result_alloca = self.fresh_value();
                self.emit(Instruction::Alloca { dest: result_alloca, ty: IrType::Ptr, size: 16, align: 0, volatile: false });
                let dest_val = self.fresh_value();
                self.emit(Instruction::Intrinsic {
                    dest: Some(dest_val),
                    op: sse_op,
                    dest_ptr: Some(result_alloca),
                    args: arg_ops,
                });
                // Return pointer to the 16-byte result (struct return)
                Some(Operand::Value(result_alloca))
            }
            // storedqu(ptr, src_ptr) - store 128 bits unaligned
            BuiltinIntrinsic::X86Storedqu => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let ptr_val = self.operand_to_value(arg_ops[0].clone());
                self.emit(Instruction::Intrinsic { dest: None, op: IntrinsicOp::Storedqu, dest_ptr: Some(ptr_val), args: vec![arg_ops[1].clone()] });
                Some(Operand::Const(IrConst::I64(0)))
            }
            // pmovmskb returns i32 scalar
            BuiltinIntrinsic::X86Pmovmskb128 => {
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let dest_val = self.fresh_value();
                self.emit(Instruction::Intrinsic {
                    dest: Some(dest_val),
                    op: IntrinsicOp::Pmovmskb128,
                    dest_ptr: None,
                    args: arg_ops,
                });
                Some(Operand::Value(dest_val))
            }
            // CRC32 operations return scalar i32/i64
            BuiltinIntrinsic::X86Crc32_8 | BuiltinIntrinsic::X86Crc32_16
            | BuiltinIntrinsic::X86Crc32_32 | BuiltinIntrinsic::X86Crc32_64 => {
                let sse_op = match intrinsic {
                    BuiltinIntrinsic::X86Crc32_8 => IntrinsicOp::Crc32_8,
                    BuiltinIntrinsic::X86Crc32_16 => IntrinsicOp::Crc32_16,
                    BuiltinIntrinsic::X86Crc32_32 => IntrinsicOp::Crc32_32,
                    BuiltinIntrinsic::X86Crc32_64 => IntrinsicOp::Crc32_64,
                    _ => unreachable!(),
                };
                let arg_ops: Vec<Operand> = args.iter().map(|a| self.lower_expr(a)).collect();
                let dest_val = self.fresh_value();
                self.emit(Instruction::Intrinsic {
                    dest: Some(dest_val),
                    op: sse_op,
                    dest_ptr: None,
                    args: arg_ops,
                });
                Some(Operand::Value(dest_val))
            }
            // __builtin_frame_address(level) / __builtin_return_address(level)
            // Only level 0 is supported; higher levels return 0.
            BuiltinIntrinsic::FrameAddress | BuiltinIntrinsic::ReturnAddress => {
                // Evaluate the level argument
                let level = if !args.is_empty() {
                    self.lower_expr(&args[0])
                } else {
                    Operand::Const(IrConst::I64(0))
                };
                // Only level 0 is supported; for other levels return NULL
                let is_level_zero = match &level {
                    Operand::Const(IrConst::I64(0)) => true,
                    Operand::Const(IrConst::I32(0)) => true,
                    _ => false,
                };
                if is_level_zero {
                    let op = if *intrinsic == BuiltinIntrinsic::FrameAddress {
                        IntrinsicOp::FrameAddress
                    } else {
                        IntrinsicOp::ReturnAddress
                    };
                    let dest_val = self.fresh_value();
                    self.emit(Instruction::Intrinsic {
                        dest: Some(dest_val),
                        op,
                        dest_ptr: None,
                        args: vec![],
                    });
                    Some(Operand::Value(dest_val))
                } else {
                    // TODO: support non-zero levels by walking the frame pointer chain
                    // Non-zero levels: return NULL (matching GCC behavior for non-optimized)
                    Some(Operand::Const(IrConst::I64(0)))
                }
            }
        }
    }

    // -----------------------------------------------------------------------
    // Intrinsic helpers
    // -----------------------------------------------------------------------

    /// Determine the operand width for a suffix-encoded intrinsic (clz, ctz, popcount, etc.).
    fn intrinsic_type_from_suffix(name: &str) -> IrType {
        if name.ends_with("ll") || name.ends_with('l') { IrType::I64 } else { IrType::I32 }
    }

    /// Lower a simple unary intrinsic (CLZ, CTZ, Popcount) that takes one integer arg.
    fn lower_unary_intrinsic(&mut self, name: &str, args: &[Expr], ir_op: IrUnaryOp) -> Option<Operand> {
        if args.is_empty() {
            return Some(Operand::Const(IrConst::I64(0)));
        }
        let arg = self.lower_expr(&args[0]);
        let ty = Self::intrinsic_type_from_suffix(name);
        let dest = self.fresh_value();
        self.emit(Instruction::UnaryOp { dest, op: ir_op, src: arg, ty });
        Some(Operand::Value(dest))
    }

    /// Lower __builtin_ffs/__builtin_ffsl/__builtin_ffsll.
    /// ffs(x) returns 0 if x == 0, otherwise (ctz(x) + 1).
    /// Synthesized as: select(x == 0, 0, ctz(x) + 1)
    fn lower_ffs_intrinsic(&mut self, name: &str, args: &[Expr]) -> Option<Operand> {
        if args.is_empty() {
            return Some(Operand::Const(IrConst::I64(0)));
        }
        let arg = self.lower_expr(&args[0]);
        let ty = Self::intrinsic_type_from_suffix(name);

        // ctz_val = ctz(x)
        let ctz_val = self.fresh_value();
        self.emit(Instruction::UnaryOp { dest: ctz_val, op: IrUnaryOp::Ctz, src: arg.clone(), ty });

        // ctz_plus_1 = ctz_val + 1
        let ctz_plus_1 = self.fresh_value();
        self.emit(Instruction::BinOp {
            dest: ctz_plus_1,
            op: IrBinOp::Add,
            lhs: Operand::Value(ctz_val),
            rhs: Operand::Const(if ty == IrType::I64 { IrConst::I64(1) } else { IrConst::I32(1) }),
            ty,
        });

        // is_zero = (x == 0)
        let is_zero = self.fresh_value();
        self.emit(Instruction::Cmp {
            dest: is_zero,
            op: IrCmpOp::Eq,
            lhs: arg,
            rhs: Operand::Const(if ty == IrType::I64 { IrConst::I64(0) } else { IrConst::I32(0) }),
            ty,
        });

        // result = select(is_zero, 0, ctz_plus_1)
        let result = self.fresh_value();
        self.emit(Instruction::Select {
            dest: result,
            cond: Operand::Value(is_zero),
            true_val: Operand::Const(if ty == IrType::I64 { IrConst::I64(0) } else { IrConst::I32(0) }),
            false_val: Operand::Value(ctz_plus_1),
            ty,
        });

        Some(Operand::Value(result))
    }

    /// Lower __builtin_bswap{16,32,64} - type determined by numeric suffix.
    fn lower_bswap_intrinsic(&mut self, name: &str, args: &[Expr]) -> Option<Operand> {
        if args.is_empty() {
            return Some(Operand::Const(IrConst::I64(0)));
        }
        let arg = self.lower_expr(&args[0]);
        let ty = if name.contains("64") {
            IrType::I64
        } else if name.contains("16") {
            IrType::I16
        } else {
            IrType::I32
        };
        let dest = self.fresh_value();
        self.emit(Instruction::UnaryOp { dest, op: IrUnaryOp::Bswap, src: arg, ty });
        Some(Operand::Value(dest))
    }

    /// Lower __builtin_parity{,l,ll} - implemented as popcount & 1.
    fn lower_parity_intrinsic(&mut self, name: &str, args: &[Expr]) -> Option<Operand> {
        if args.is_empty() {
            return Some(Operand::Const(IrConst::I64(0)));
        }
        let arg = self.lower_expr(&args[0]);
        let ty = Self::intrinsic_type_from_suffix(name);
        let pop = self.fresh_value();
        self.emit(Instruction::UnaryOp { dest: pop, op: IrUnaryOp::Popcount, src: arg, ty });
        let dest = self.emit_binop_val(IrBinOp::And, Operand::Value(pop), Operand::Const(IrConst::I64(1)), ty);
        Some(Operand::Value(dest))
    }

    /// Lower __builtin_clrsb{,l,ll}(x) - count leading redundant sign bits.
    /// Computes: clz(x ^ negate(x >>> (bits-1))) - 1
    /// Uses logical shift + negate to create sign mask (avoids AShr 32-bit backend issues).
    fn lower_clrsb_intrinsic(&mut self, name: &str, args: &[Expr]) -> Option<Operand> {
        if args.is_empty() {
            return Some(Operand::Const(IrConst::I64(0)));
        }
        let arg = self.lower_expr(&args[0]);
        let ty = Self::intrinsic_type_from_suffix(name);
        let bits = if ty == IrType::I64 { 63i64 } else { 31i64 };

        // Extract sign bit using logical shift right: sign_bit = x >>> (bits)
        // This gives 0 for positive, 1 for negative
        let sign_bit = self.fresh_value();
        self.emit(Instruction::BinOp {
            dest: sign_bit,
            op: IrBinOp::LShr,
            lhs: arg.clone(),
            rhs: Operand::Const(IrConst::I64(bits)),
            ty,
        });

        // Negate sign bit to get mask: 0 -> 0, 1 -> -1 (all ones)
        let sign_mask = self.fresh_value();
        self.emit(Instruction::UnaryOp {
            dest: sign_mask,
            op: IrUnaryOp::Neg,
            src: Operand::Value(sign_bit),
            ty,
        });

        // XOR x with sign_mask: positive unchanged, negative gets ~x
        let xored = self.fresh_value();
        self.emit(Instruction::BinOp {
            dest: xored,
            op: IrBinOp::Xor,
            lhs: arg,
            rhs: Operand::Value(sign_mask),
            ty,
        });

        // CLZ of the XOR result
        let clz_val = self.fresh_value();
        self.emit(Instruction::UnaryOp {
            dest: clz_val,
            op: IrUnaryOp::Clz,
            src: Operand::Value(xored),
            ty,
        });

        // Subtract 1 (the sign bit itself is not counted)
        let result = self.emit_binop_val(
            IrBinOp::Sub,
            Operand::Value(clz_val),
            Operand::Const(IrConst::I64(1)),
            IrType::I32,
        );

        Some(Operand::Value(result))
    }

    // =========================================================================
    // Overflow-checking arithmetic builtins
    // =========================================================================

    /// Lower __builtin_{add,sub,mul}_overflow(a, b, result_ptr) and type-specific variants.
    ///
    /// These builtins perform an arithmetic operation, store the result through
    /// the pointer, and return 1 (bool true) if the operation overflowed.
    fn lower_overflow_builtin(&mut self, name: &str, args: &[Expr], op: IrBinOp) -> Option<Operand> {
        if args.len() < 3 {
            return Some(Operand::Const(IrConst::I64(0)));
        }

        // Determine the result type from the builtin name or from the pointer argument type.
        // Type-specific variants: __builtin_sadd_overflow (signed int), __builtin_uaddll_overflow (unsigned long long)
        // Generic variants: __builtin_add_overflow - type from pointer arg
        let (result_ir_ty, is_signed) = self.overflow_result_type(name, &args[2]);

        // Detect generic variants: __builtin_{add,sub,mul}_overflow
        // (as opposed to type-specific like __builtin_sadd_overflow, __builtin_uaddll_overflow)
        let is_generic = name == "__builtin_add_overflow"
            || name == "__builtin_sub_overflow"
            || name == "__builtin_mul_overflow";

        // Lower the two operands
        let lhs_raw = self.lower_expr(&args[0]);
        let rhs_raw = self.lower_expr(&args[1]);

        // For generic variants, operands may have narrower/different types than the
        // result. Cast each operand respecting its own signedness (sign-extend for
        // signed, zero-extend for unsigned) so that e.g. (int)-16 is correctly
        // represented in u128.
        let lhs_src_ctype = self.expr_ctype(&args[0]);
        let rhs_src_ctype = self.expr_ctype(&args[1]);
        let (lhs_val, rhs_val) = if is_generic {
            let lhs_src_ir = IrType::from_ctype(&lhs_src_ctype);
            let rhs_src_ir = IrType::from_ctype(&rhs_src_ctype);
            let lhs = if lhs_src_ir != result_ir_ty {
                Operand::Value(self.emit_cast_val(lhs_raw, lhs_src_ir, result_ir_ty))
            } else {
                lhs_raw
            };
            let rhs = if rhs_src_ir != result_ir_ty {
                Operand::Value(self.emit_cast_val(rhs_raw, rhs_src_ir, result_ir_ty))
            } else {
                rhs_raw
            };
            (lhs, rhs)
        } else {
            (lhs_raw, rhs_raw)
        };

        // Get the result pointer
        let result_ptr_val = self.lower_expr(&args[2]);
        let result_ptr = self.operand_to_value(result_ptr_val);

        // Perform the operation in the result type (operands are now properly widened)
        let result = self.emit_binop_val(op, lhs_val.clone(), rhs_val.clone(), result_ir_ty);

        // Store the (possibly truncated/wrapped) result
        self.emit(Instruction::Store { val: Operand::Value(result), ptr: result_ptr, ty: result_ir_ty,
         seg_override: AddressSpace::Default });

        // Compute the overflow flag.
        // For generic variants with signed source(s) and unsigned result type, we use
        // infinite-precision semantics: a negative mathematical result can't fit in an
        // unsigned type. We detect this by checking the sign bit of the result.
        let any_source_signed = is_generic
            && (lhs_src_ctype.is_signed() || rhs_src_ctype.is_signed());
        let overflow = if is_signed {
            self.compute_signed_overflow(op, lhs_val, rhs_val, result, result_ir_ty)
        } else if any_source_signed {
            // Generic variant with signed source(s) and unsigned result.
            // When all source types are narrower than the result type, both
            // operands fit in the signed variant of the result type, so the
            // operation in the unsigned result type is exact (no unsigned wrapping
            // can occur). Overflow only happens if the mathematical result is
            // negative (detected via sign bit).
            // When a source is as wide as the result type, unsigned wrapping CAN
            // occur, so we must also check for standard unsigned overflow.
            let lhs_src_ir = IrType::from_ctype(&lhs_src_ctype);
            let rhs_src_ir = IrType::from_ctype(&rhs_src_ctype);
            let sources_narrower = lhs_src_ir.size() < result_ir_ty.size()
                && rhs_src_ir.size() < result_ir_ty.size();
            if sources_narrower {
                // All sources narrower: only negative result can overflow unsigned
                self.check_result_sign_bit(result, result_ir_ty)
            } else {
                // At least one source is as wide as result: check both conditions
                let sign_ov = self.check_result_sign_bit(result, result_ir_ty);
                let unsigned_ov = self.compute_unsigned_overflow(
                    op, lhs_val, rhs_val, result, result_ir_ty,
                );
                self.emit_binop_val(
                    IrBinOp::Or,
                    Operand::Value(sign_ov),
                    Operand::Value(unsigned_ov),
                    result_ir_ty,
                )
            }
        } else {
            self.compute_unsigned_overflow(op, lhs_val, rhs_val, result, result_ir_ty)
        };

        Some(Operand::Value(overflow))
    }

    /// Determine the result IrType and signedness for an overflow builtin.
    fn overflow_result_type(&self, name: &str, result_ptr_expr: &Expr) -> (IrType, bool) {
        // Check type-specific variants first
        // __builtin_sadd_overflow -> signed int
        // __builtin_saddl_overflow -> signed long
        // __builtin_saddll_overflow -> signed long long
        // __builtin_uadd_overflow -> unsigned int
        // etc.
        if name.starts_with("__builtin_s") && name != "__builtin_sub_overflow" {
            // Signed type-specific variant
            let is_signed = true;
            let ty = if name.ends_with("ll_overflow") {
                IrType::I64
            } else if name.ends_with("l_overflow") {
                IrType::I64  // long = i64 on 64-bit
            } else {
                IrType::I32
            };
            return (ty, is_signed);
        }
        if name.starts_with("__builtin_u") {
            // Unsigned type-specific variant
            let is_signed = false;
            let ty = if name.ends_with("ll_overflow") {
                IrType::U64
            } else if name.ends_with("l_overflow") {
                IrType::U64
            } else {
                IrType::U32
            };
            return (ty, is_signed);
        }

        // Generic variant: determine type from the pointer argument's pointee type
        let result_ctype = self.expr_ctype(result_ptr_expr);
        if let CType::Pointer(pointee, _) = &result_ctype {
            let is_signed = pointee.is_signed();
            let ir_ty = IrType::from_ctype(pointee);
            return (ir_ty, is_signed);
        }

        // If we can't determine the type (e.g., &local), try to get the
        // pointee from the expression structure
        if let Expr::AddressOf(inner, _) = result_ptr_expr {
            let inner_ctype = self.expr_ctype(inner);
            let is_signed = inner_ctype.is_signed();
            let ir_ty = IrType::from_ctype(&inner_ctype);
            return (ir_ty, is_signed);
        }

        // Default to signed long (i64) if we can't determine
        (IrType::I64, true)
    }

    /// Compute signed overflow flag for add/sub/mul.
    fn compute_signed_overflow(
        &mut self,
        op: IrBinOp,
        lhs: Operand,
        rhs: Operand,
        result: Value,
        ty: IrType,
    ) -> Value {
        let bits = (ty.size() * 8) as i64;
        match op {
            IrBinOp::Add => {
                // Signed add overflow: ((result ^ lhs) & (result ^ rhs)) < 0
                // i.e., overflow if both operands have same sign and result has different sign
                let xor_lhs = self.emit_binop_val(IrBinOp::Xor, Operand::Value(result), lhs, ty);
                let xor_rhs = self.emit_binop_val(IrBinOp::Xor, Operand::Value(result), rhs, ty);
                let and_val = self.emit_binop_val(IrBinOp::And, Operand::Value(xor_lhs), Operand::Value(xor_rhs), ty);
                // Check sign bit: shift right by (bits-1) and check if non-zero
                let shifted = self.emit_binop_val(IrBinOp::AShr, Operand::Value(and_val), Operand::Const(IrConst::I64(bits - 1)), ty);
                // The shifted value is all-ones (-1) if overflow, all-zeros (0) if not
                // Convert to 0/1 by AND with 1
                self.emit_binop_val(IrBinOp::And, Operand::Value(shifted), Operand::Const(IrConst::I64(1)), ty)
            }
            IrBinOp::Sub => {
                // Signed sub overflow: ((lhs ^ rhs) & (result ^ lhs)) < 0
                // overflow if operands have different signs and result sign differs from lhs
                let xor_ops = self.emit_binop_val(IrBinOp::Xor, lhs.clone(), rhs, ty);
                let xor_res = self.emit_binop_val(IrBinOp::Xor, Operand::Value(result), lhs, ty);
                let and_val = self.emit_binop_val(IrBinOp::And, Operand::Value(xor_ops), Operand::Value(xor_res), ty);
                let shifted = self.emit_binop_val(IrBinOp::AShr, Operand::Value(and_val), Operand::Const(IrConst::I64(bits - 1)), ty);
                self.emit_binop_val(IrBinOp::And, Operand::Value(shifted), Operand::Const(IrConst::I64(1)), ty)
            }
            IrBinOp::Mul => {
                // For signed multiply overflow, we widen to double width, multiply, then
                // check if the result fits in the original width by checking that
                // sign-extending the truncated result gives back the full result.
                let wide_ty = Self::double_width_type(ty, true);
                let lhs_wide = self.emit_cast_val(lhs, ty, wide_ty);
                let rhs_wide = self.emit_cast_val(rhs, ty, wide_ty);
                let wide_result = self.emit_binop_val(IrBinOp::Mul, Operand::Value(lhs_wide), Operand::Value(rhs_wide), wide_ty);
                // Truncate back to original width
                let truncated = self.emit_cast_val(Operand::Value(wide_result), wide_ty, ty);
                // Sign-extend truncated back to wide type
                let sign_extended = self.emit_cast_val(Operand::Value(truncated), ty, wide_ty);
                // Overflow if wide_result != sign_extended
                self.emit_cmp_val(IrCmpOp::Ne, Operand::Value(wide_result), Operand::Value(sign_extended), wide_ty)
            }
            _ => unreachable!("overflow only for add/sub/mul"),
        }
    }

    /// Compute unsigned overflow flag for add/sub/mul.
    fn compute_unsigned_overflow(
        &mut self,
        op: IrBinOp,
        lhs: Operand,
        rhs: Operand,
        result: Value,
        ty: IrType,
    ) -> Value {
        // Make sure we're comparing in unsigned type
        let unsigned_ty = ty.to_unsigned();
        match op {
            IrBinOp::Add => {
                // Unsigned add overflow: result < lhs (wrapping means the sum is smaller)
                self.emit_cmp_val(IrCmpOp::Ult, Operand::Value(result), lhs, unsigned_ty)
            }
            IrBinOp::Sub => {
                // Unsigned sub overflow: lhs < rhs (borrow occurred)
                self.emit_cmp_val(IrCmpOp::Ult, lhs, rhs, unsigned_ty)
            }
            IrBinOp::Mul => {
                // Unsigned multiply overflow: widen to double width, multiply, check upper half is zero
                let wide_ty = Self::double_width_type(ty, false);
                let lhs_wide = self.emit_cast_val(lhs, unsigned_ty, wide_ty);
                let rhs_wide = self.emit_cast_val(rhs, unsigned_ty, wide_ty);
                let wide_result = self.emit_binop_val(IrBinOp::Mul, Operand::Value(lhs_wide), Operand::Value(rhs_wide), wide_ty);
                // Truncate back to original width
                let truncated = self.emit_cast_val(Operand::Value(wide_result), wide_ty, unsigned_ty);
                // Zero-extend truncated back to wide type
                let zero_extended = self.emit_cast_val(Operand::Value(truncated), unsigned_ty, wide_ty);
                // Overflow if wide_result != zero_extended
                self.emit_cmp_val(IrCmpOp::Ne, Operand::Value(wide_result), Operand::Value(zero_extended), wide_ty)
            }
            _ => unreachable!("overflow only for add/sub/mul"),
        }
    }

    /// Check if the sign bit of `result` is set when interpreted in `ty`.
    /// Returns a value that is non-zero (1) if the sign bit is set, 0 otherwise.
    /// Used to detect overflow when a signed source produces a negative result
    /// that is being stored into an unsigned result type.
    fn check_result_sign_bit(&mut self, result: Value, ty: IrType) -> Value {
        let bits = (ty.size() * 8) as i64;
        // Arithmetic shift right by (bits-1): all-ones if negative, all-zeros if positive
        let shifted = self.emit_binop_val(
            IrBinOp::AShr,
            Operand::Value(result),
            Operand::Const(IrConst::I64(bits - 1)),
            ty,
        );
        // AND with 1 to get 0 or 1
        self.emit_binop_val(
            IrBinOp::And,
            Operand::Value(shifted),
            Operand::Const(IrConst::I64(1)),
            ty,
        )
    }

    /// Get the double-width integer type for overflow multiply detection.
    fn double_width_type(ty: IrType, signed: bool) -> IrType {
        match ty {
            IrType::I8 | IrType::U8 => if signed { IrType::I16 } else { IrType::U16 },
            IrType::I16 | IrType::U16 => if signed { IrType::I32 } else { IrType::U32 },
            IrType::I32 | IrType::U32 => if signed { IrType::I64 } else { IrType::U64 },
            IrType::I64 | IrType::U64 => if signed { IrType::I128 } else { IrType::U128 },
            // For 128-bit, we can't widen further; use a different strategy
            // (but this is extremely rare in practice)
            _ => if signed { IrType::I128 } else { IrType::U128 },
        }
    }

    // =========================================================================
    // Floating-point classification builtins
    // =========================================================================

    /// Helper: get the type of the last argument for fp classification builtins.
    fn lower_fp_classify_arg(&mut self, args: &[Expr]) -> (IrType, Operand) {
        let arg_expr = args.last().unwrap();
        let arg_ty = self.get_expr_type(arg_expr);
        let arg_val = self.lower_expr(arg_expr);
        (arg_ty, arg_val)
    }

    /// Bitcast a float/double to its integer bit representation via memory.
    /// F32 -> I32, F64/F128 -> I64. Uses alloca+store+load for a true bitcast.
    // TODO: F128 (long double) is currently stored as F64 at the backend level,
    // so we treat it as F64 for bitcast purposes (8 bytes, I64 result).
    // When true F128 support is added, this must handle 16-byte bitcast.
    fn bitcast_float_to_int(&mut self, val: Operand, float_ty: IrType) -> (Operand, IrType) {
        let (int_ty, store_ty, size) = match float_ty {
            IrType::F32 => (IrType::I32, IrType::F32, 4),
            // F128 is stored as F64 at backend level, treat identically
            _ => (IrType::I64, IrType::F64, 8),
        };

        let tmp_alloca = self.fresh_value();
        self.emit(Instruction::Alloca { dest: tmp_alloca, size, ty: store_ty, align: 0, volatile: false });

        let val_v = self.operand_to_value(val);
        self.emit(Instruction::Store { val: Operand::Value(val_v), ptr: tmp_alloca, ty: store_ty, seg_override: AddressSpace::Default });

        let result = self.fresh_value();
        self.emit(Instruction::Load { dest: result, ptr: tmp_alloca, ty: int_ty, seg_override: AddressSpace::Default });

        (Operand::Value(result), int_ty)
    }

    /// Floating-point bit-layout constants for classification.
    /// Returns (abs_mask, exp_only, exp_shift, exp_field_max, mant_mask).
    // TODO: F128 (long double) is currently stored as F64 at the backend level,
    // so we use F64 masks for it. When true F128 support is added, this must be
    // updated with IEEE 754 binary128 masks (15-bit exponent, 112-bit mantissa).
    pub(super) fn fp_masks(float_ty: IrType) -> (i64, i64, i64, i64, i64) {
        match float_ty {
            IrType::F32 => (
                0x7FFFFFFF_i64,
                0x7F800000_i64,
                23,
                0xFF,
                0x007FFFFF_i64,
            ),
            // F128 uses F64 masks because the backend stores long double as double
            _ => (
                0x7FFFFFFFFFFFFFFF_i64,
                0x7FF0000000000000_u64 as i64,
                52,
                0x7FF,
                0x000FFFFFFFFFFFFF_u64 as i64,
            ),
        }
    }

    /// Compute the absolute value (sign-stripped) of float bits: bits & abs_mask.
    fn fp_abs_bits(&mut self, bits: &Operand, int_ty: IrType, abs_mask: i64) -> Value {
        self.emit_binop_val(IrBinOp::And, bits.clone(), Operand::Const(IrConst::I64(abs_mask)), int_ty)
    }

    /// Lower __builtin_fpclassify(nan_val, inf_val, norm_val, subnorm_val, zero_val, x).
    /// Uses arithmetic select: result = sum of (class_val[i] * is_class[i]).
    fn lower_builtin_fpclassify(&mut self, args: &[Expr]) -> Option<Operand> {
        if args.len() < 6 {
            return Some(Operand::Const(IrConst::I64(0)));
        }

        let class_vals: Vec<_> = (0..5).map(|i| self.lower_expr(&args[i])).collect();
        let arg_ty = self.get_expr_type(&args[5]);
        let arg_val = self.lower_expr(&args[5]);

        let (bits, int_ty) = self.bitcast_float_to_int(arg_val, arg_ty);
        let (_abs_mask, _exp_only, exp_shift, exp_field_max, mant_mask) = Self::fp_masks(arg_ty);

        let shifted = self.emit_binop_val(IrBinOp::LShr, bits.clone(), Operand::Const(IrConst::I64(exp_shift)), int_ty);
        let exponent = self.emit_binop_val(IrBinOp::And, Operand::Value(shifted), Operand::Const(IrConst::I64(exp_field_max)), int_ty);
        let mantissa = self.emit_binop_val(IrBinOp::And, bits, Operand::Const(IrConst::I64(mant_mask)), int_ty);

        let exp_is_max = self.emit_cmp_val(IrCmpOp::Eq, Operand::Value(exponent), Operand::Const(IrConst::I64(exp_field_max)), int_ty);
        let exp_is_zero = self.emit_cmp_val(IrCmpOp::Eq, Operand::Value(exponent), Operand::Const(IrConst::I64(0)), int_ty);
        let mant_is_zero = self.emit_cmp_val(IrCmpOp::Eq, Operand::Value(mantissa), Operand::Const(IrConst::I64(0)), int_ty);
        let mant_not_zero = self.emit_cmp_val(IrCmpOp::Ne, Operand::Value(mantissa), Operand::Const(IrConst::I64(0)), int_ty);

        let is_nan = self.emit_binop_val(IrBinOp::And, Operand::Value(exp_is_max), Operand::Value(mant_not_zero), IrType::I32);
        let is_inf = self.emit_binop_val(IrBinOp::And, Operand::Value(exp_is_max), Operand::Value(mant_is_zero), IrType::I32);
        let is_zero = self.emit_binop_val(IrBinOp::And, Operand::Value(exp_is_zero), Operand::Value(mant_is_zero), IrType::I32);
        let is_subnorm = self.emit_binop_val(IrBinOp::And, Operand::Value(exp_is_zero), Operand::Value(mant_not_zero), IrType::I32);

        let mut special = is_nan;
        for &flag in &[is_inf, is_zero, is_subnorm] {
            special = self.emit_binop_val(IrBinOp::Or, Operand::Value(special), Operand::Value(flag), IrType::I32);
        }
        let is_normal = self.emit_binop_val(IrBinOp::Xor, Operand::Value(special), Operand::Const(IrConst::I64(1)), IrType::I32);

        // Order: nan=0, inf=1, normal=2, subnormal=3, zero=4
        let class_flags = [is_nan, is_inf, is_normal, is_subnorm, is_zero];
        let contribs: Vec<_> = class_vals.iter().zip(class_flags.iter())
            .map(|(val, &flag)| (val.clone(), flag))
            .collect();
        let mut result = self.emit_binop_val(IrBinOp::Mul, contribs[0].0.clone(), Operand::Value(contribs[0].1), IrType::I64);
        for &(ref val, flag) in &contribs[1..] {
            let contrib = self.emit_binop_val(IrBinOp::Mul, val.clone(), Operand::Value(flag), IrType::I64);
            result = self.emit_binop_val(IrBinOp::Add, Operand::Value(result), Operand::Value(contrib), IrType::I64);
        }

        Some(Operand::Value(result))
    }

    /// Shared setup for FP classification builtins: validates args, lowers the
    /// float argument, bitcasts to integer bits, and retrieves the FP masks.
    /// The closure receives (self, bits, arg_ty, int_ty, masks) and returns the result Value.
    ///
    // TODO: F128 is stored as F64 at the backend, so bitcast and masks both use F64 layout.
    fn lower_fp_classify_with<F>(&mut self, args: &[Expr], classify: F) -> Option<Operand>
    where
        F: FnOnce(&mut Self, Operand, IrType, IrType, (i64, i64, i64, i64, i64)) -> Value,
    {
        if args.is_empty() { return Some(Operand::Const(IrConst::I64(0))); }
        let (arg_ty, arg_val) = self.lower_fp_classify_arg(args);
        let (bits, int_ty) = self.bitcast_float_to_int(arg_val, arg_ty);
        let masks = Self::fp_masks(arg_ty);
        let result = classify(self, bits, arg_ty, int_ty, masks);
        Some(Operand::Value(result))
    }

    /// __builtin_isnan(x) -> 1 if x is NaN. NaN: abs(bits) > exp_only.
    fn lower_builtin_isnan(&mut self, args: &[Expr]) -> Option<Operand> {
        self.lower_fp_classify_with(args, |s, bits, _, int_ty, (abs_mask, exp_only, _, _, _)| {
            let abs_val = s.fp_abs_bits(&bits, int_ty, abs_mask);
            s.emit_cmp_val(IrCmpOp::Ugt, Operand::Value(abs_val), Operand::Const(IrConst::I64(exp_only)), int_ty)
        })
    }

    /// __builtin_isinf(x) -> nonzero if +/-infinity. Inf: abs(bits) == exp_only.
    fn lower_builtin_isinf(&mut self, args: &[Expr]) -> Option<Operand> {
        self.lower_fp_classify_with(args, |s, bits, _, int_ty, (abs_mask, exp_only, _, _, _)| {
            let abs_val = s.fp_abs_bits(&bits, int_ty, abs_mask);
            s.emit_cmp_val(IrCmpOp::Eq, Operand::Value(abs_val), Operand::Const(IrConst::I64(exp_only)), int_ty)
        })
    }

    /// __builtin_isfinite(x) -> 1 if finite. Finite: (bits & exp_only) != exp_only.
    fn lower_builtin_isfinite(&mut self, args: &[Expr]) -> Option<Operand> {
        self.lower_fp_classify_with(args, |s, bits, _, int_ty, (_, exp_only, _, _, _)| {
            let exp_bits = s.emit_binop_val(IrBinOp::And, bits, Operand::Const(IrConst::I64(exp_only)), int_ty);
            s.emit_cmp_val(IrCmpOp::Ne, Operand::Value(exp_bits), Operand::Const(IrConst::I64(exp_only)), int_ty)
        })
    }

    /// __builtin_isnormal(x) -> 1 if normal. Normal: exp != 0 AND exp != max.
    fn lower_builtin_isnormal(&mut self, args: &[Expr]) -> Option<Operand> {
        self.lower_fp_classify_with(args, |s, bits, _, int_ty, (_, exp_only, exp_shift, exp_field_max, _)| {
            let exp_bits = s.emit_binop_val(IrBinOp::And, bits, Operand::Const(IrConst::I64(exp_only)), int_ty);
            let exponent = s.emit_binop_val(IrBinOp::LShr, Operand::Value(exp_bits), Operand::Const(IrConst::I64(exp_shift)), int_ty);
            let exp_nonzero = s.emit_cmp_val(IrCmpOp::Ne, Operand::Value(exponent), Operand::Const(IrConst::I64(0)), int_ty);
            let exp_not_max = s.emit_cmp_val(IrCmpOp::Ne, Operand::Value(exponent), Operand::Const(IrConst::I64(exp_field_max)), int_ty);
            s.emit_binop_val(IrBinOp::And, Operand::Value(exp_nonzero), Operand::Value(exp_not_max), IrType::I32)
        })
    }

    /// __builtin_signbit(x) -> nonzero if sign bit set.
    fn lower_builtin_signbit(&mut self, args: &[Expr]) -> Option<Operand> {
        self.lower_fp_classify_with(args, |s, bits, arg_ty, int_ty, _| {
            let sign_shift = if arg_ty == IrType::F32 { 31_i64 } else { 63_i64 };
            let shifted = s.emit_binop_val(IrBinOp::LShr, bits, Operand::Const(IrConst::I64(sign_shift)), int_ty);
            s.emit_binop_val(IrBinOp::And, Operand::Value(shifted), Operand::Const(IrConst::I64(1)), int_ty)
        })
    }

    /// __builtin_isinf_sign(x) -> -1 if -inf, +1 if +inf, 0 otherwise.
    fn lower_builtin_isinf_sign(&mut self, args: &[Expr]) -> Option<Operand> {
        self.lower_fp_classify_with(args, |s, bits, arg_ty, int_ty, (abs_mask, exp_only, _, _, _)| {
            let sign_shift = if arg_ty == IrType::F32 { 31_i64 } else { 63_i64 };
            let abs_val = s.fp_abs_bits(&bits, int_ty, abs_mask);
            let is_inf = s.emit_cmp_val(IrCmpOp::Eq, Operand::Value(abs_val), Operand::Const(IrConst::I64(exp_only)), int_ty);
            let sign_shifted = s.emit_binop_val(IrBinOp::LShr, bits, Operand::Const(IrConst::I64(sign_shift)), int_ty);
            let sign_bit = s.emit_binop_val(IrBinOp::And, Operand::Value(sign_shifted), Operand::Const(IrConst::I64(1)), int_ty);
            let sign_x2 = s.emit_binop_val(IrBinOp::Mul, Operand::Value(sign_bit), Operand::Const(IrConst::I64(2)), IrType::I64);
            let direction = s.emit_binop_val(IrBinOp::Sub, Operand::Const(IrConst::I64(1)), Operand::Value(sign_x2), IrType::I64);
            s.emit_binop_val(IrBinOp::Mul, Operand::Value(is_inf), Operand::Value(direction), IrType::I64)
        })
    }
}

/// Classify a CType according to GCC's __builtin_classify_type conventions.
/// Returns the GCC type class integer:
///   0 = no_type_class (void)
///   1 = integer_type_class
///   2 = char_type_class
///   3 = enumeral_type_class
///   4 = boolean_type_class
///   5 = pointer_type_class
///   6 = reference_type_class (C++ only, not used in C)
///   8 = real_type_class
///   9 = complex_type_class
///  12 = record_type_class (struct)
///  13 = union_type_class
///  14 = array_type_class
///  16 = lang_type_class
fn classify_ctype(ty: &CType) -> i64 {
    // GCC __builtin_classify_type returns integer type classes.
    // GCC treats char, _Bool, and enum as integer_type_class (1) in practice.
    match ty {
        CType::Void => 0,            // no_type_class
        CType::Bool
        | CType::Char | CType::UChar
        | CType::Short | CType::UShort
        | CType::Int | CType::UInt
        | CType::Long | CType::ULong
        | CType::LongLong | CType::ULongLong
        | CType::Int128 | CType::UInt128
        | CType::Enum(_) => 1,       // integer_type_class
        CType::Float | CType::Double | CType::LongDouble => 8, // real_type_class
        CType::ComplexFloat | CType::ComplexDouble
        | CType::ComplexLongDouble => 9, // complex_type_class
        CType::Pointer(_, _) => 5,      // pointer_type_class
        CType::Array(_, _) => 5,     // GCC decays arrays to pointers
        CType::Function(_) => 5,     // function decays to pointer
        CType::Struct(_) => 12,      // record_type_class
        CType::Union(_) => 13,       // union_type_class
    }
}

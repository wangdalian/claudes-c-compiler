//! x86-64 InlineAsmEmitter implementation: constraint classification, register
//! allocation, operand loading/storing, and template substitution for inline asm.

use crate::ir::ir::*;
use crate::common::types::IrType;
use crate::backend::state::CodegenState;
use crate::backend::inline_asm::{InlineAsmEmitter, AsmOperandKind, AsmOperand};
use super::codegen::X86Codegen;

/// x86-64 scratch registers for inline asm "r" constraints (caller-saved, not rax/rsp/rbp).
const X86_GP_SCRATCH: &[&str] = &["rcx", "rdx", "rsi", "rdi", "r8", "r9", "r10", "r11"];

/// x86-64 scratch XMM registers for inline asm "x" constraints.
/// Includes all 16 SSE registers (xmm0-xmm15). xmm16-xmm31 require AVX-512
/// EVEX encoding and must not be used without explicit AVX-512 support.
const X86_XMM_SCRATCH: &[&str] = &[
    "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7",
    "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14", "xmm15",
];

impl InlineAsmEmitter for X86Codegen {
    fn asm_state(&mut self) -> &mut CodegenState { &mut self.state }

    // TODO: ARM and RISC-V backends should also support multi-alternative constraint
    // parsing (e.g., "rm", "ri") similar to the x86 implementation below. Currently
    // they only recognize single-alternative constraints.
    fn classify_constraint(&self, constraint: &str) -> AsmOperandKind {
        let c = constraint.trim_start_matches(|c: char| c == '=' || c == '+' || c == '&');
        // Explicit register constraint from register variable: {regname}
        // Generated by the IR lowering for `register long x __asm__("regname")`.
        if c.starts_with('{') && c.ends_with('}') {
            let reg_name = &c[1..c.len()-1];
            return AsmOperandKind::Specific(reg_name.to_string());
        }
        // GCC condition code output: =@cc<cond> (e.g. =@cce, =@ccne, =@ccs)
        if let Some(cond) = c.strip_prefix("@cc") {
            return AsmOperandKind::ConditionCode(cond.to_string());
        }
        // Check for tied operand (all digits)
        if !c.is_empty() && c.chars().all(|ch| ch.is_ascii_digit()) {
            if let Ok(n) = c.parse::<usize>() {
                return AsmOperandKind::Tied(n);
            }
        }
        // x87 FPU stack constraints: "t" = st(0), "u" = st(1)
        // These are always single-character constraints, so check before multi-alt parsing.
        if c == "t" {
            return AsmOperandKind::X87St0;
        }
        if c == "u" {
            return AsmOperandKind::X87St1;
        }

        // GCC allows multi-alternative constraints like "Ir" (immediate or register),
        // "rm" (register or memory), "qm" (byte register or memory), etc.
        // Parse each character as a constraint alternative and pick the best one.
        // Priority: specific register > GP register > FP register > memory > immediate
        // Registers are preferred over memory for performance. Immediate is the fallback
        // because the shared framework promotes GpReg to Immediate when the value is
        // a compile-time constant and the constraint allows it (see emit_inline_asm_common).
        // NOTE: The immediate letters here are x86-specific and intentionally a superset
        // of the architecture-neutral set in constraint_has_immediate_alt().
        let mut has_gp = false;
        let mut has_fp = false;
        let mut has_mem = false;
        let mut has_imm = false;
        let mut has_qreg = false;
        let mut specific: Option<String> = None;

        for ch in c.chars() {
            match ch {
                'r' | 'q' | 'R' | 'l' => has_gp = true,
                'Q' => has_qreg = true,
                'g' => { has_gp = true; has_mem = true; has_imm = true; } // "general operand"
                'x' | 'v' | 'Y' => has_fp = true,
                'm' | 'o' | 'V' | 'p' => has_mem = true, // 'p' = valid memory address
                'i' | 'I' | 'n' | 'N' | 'e' | 'E' | 'K' | 'M' | 'G' | 'H' | 'J' | 'L' | 'O' => has_imm = true,
                'a' if specific.is_none() => specific = Some("rax".to_string()),
                'b' if specific.is_none() => specific = Some("rbx".to_string()),
                'c' if specific.is_none() => specific = Some("rcx".to_string()),
                'd' if specific.is_none() => specific = Some("rdx".to_string()),
                'S' if specific.is_none() => specific = Some("rsi".to_string()),
                'D' if specific.is_none() => specific = Some("rdi".to_string()),
                // GCC "A" constraint: on x86-64, equivalent to "a" (rax).
                // On x86-32, it means edx:eax pair for 64-bit values, but
                // we handle x86-64 only here (i686 backend has its own classify_constraint).
                'A' if specific.is_none() => specific = Some("rax".to_string()),
                _ => {}
            }
        }

        // Return the most appropriate classification.
        // For multi-alternative constraints like "Ir", the IR lowering will have
        // already evaluated the operand as a constant if possible, so the actual
        // substitution code will check imm_value to decide whether to emit $N or %reg.
        if let Some(reg) = specific {
            AsmOperandKind::Specific(reg)
        } else if has_gp {
            AsmOperandKind::GpReg
        } else if has_qreg {
            AsmOperandKind::QReg
        } else if has_fp {
            AsmOperandKind::FpReg
        } else if has_mem {
            AsmOperandKind::Memory
        } else if has_imm {
            AsmOperandKind::Immediate
        } else {
            AsmOperandKind::GpReg
        }
    }

    fn setup_operand_metadata(&self, op: &mut AsmOperand, val: &Operand, _is_output: bool) {
        if matches!(op.kind, AsmOperandKind::Memory) {
            match val {
                Operand::Value(v) => {
                    if let Some(slot) = self.state.get_slot(v.0) {
                        if self.state.is_alloca(v.0) {
                            if self.state.alloca_over_align(v.0).is_some() {
                                // Over-aligned alloca (e.g., __attribute__((aligned(32)))):
                                // the raw rbp-relative offset is NOT guaranteed to be aligned.
                                // Leave mem_addr empty so resolve_memory_operand computes the
                                // runtime-aligned address into a scratch register.
                                op.mem_addr = String::new();
                            } else {
                                // Normal alloca: stack slot IS the memory location
                                op.mem_addr = format!("{}(%rbp)", slot.0);
                            }
                        } else {
                            // Non-alloca: slot holds a pointer that needs indirection.
                            // Mark with empty mem_addr; resolve_memory_operand will load
                            // the pointer into a register and set up the indirect address.
                            op.mem_addr = String::new();
                        }
                    }
                }
                _ => {}
            }
        }
        // Extract immediate constant value.
        // For pure Immediate constraints, this provides the value for $N substitution.
        if matches!(op.kind, AsmOperandKind::Immediate) {
            if let Operand::Const(c) = val {
                op.imm_value = c.to_i64();
            }
        }
    }

    fn resolve_memory_operand(&mut self, op: &mut AsmOperand, val: &Operand, excluded: &[String]) -> bool {
        // If mem_addr is already set (normal alloca case), nothing to do
        if !op.mem_addr.is_empty() {
            return false;
        }
        // If we have a symbol name for this memory operand, use a direct RIP-relative
        // symbol reference. This is critical for correctness when the inline asm template
        // adds a segment prefix (e.g., %%gs:) before the operand — using a register
        // indirect like (%rcx) would use the absolute address as a segment offset,
        // but a direct symbol(%rip) reference provides the correct per-CPU offset.
        if let Some(ref sym) = op.imm_symbol {
            op.mem_addr = format!("{}(%rip)", sym);
            return false; // no scratch register used
        }
        // Load the pointer value into a temporary register for indirect addressing.
        // Each memory operand gets its own unique register via assign_scratch_reg,
        // so multiple "=m" outputs don't overwrite each other's addresses.
        match val {
            Operand::Value(v) => {
                if let Some(slot) = self.state.get_slot(v.0) {
                    let tmp_reg = self.assign_scratch_reg(&AsmOperandKind::GpReg, excluded);
                    if self.state.is_alloca(v.0) {
                        if let Some(align) = self.state.alloca_over_align(v.0) {
                            // Over-aligned alloca: compute runtime-aligned address.
                            // The raw stack slot has extra padding; the aligned address
                            // is (slot_addr + align-1) & ~(align-1).
                            self.state.out.emit_instr_rbp_reg("    leaq", slot.0, &tmp_reg);
                            self.state.out.emit_instr_imm_reg("    addq", (align - 1) as i64, &tmp_reg);
                            self.state.out.emit_instr_imm_reg("    andq", -(align as i64), &tmp_reg);
                        } else {
                            // Normal alloca (shouldn't reach here since setup_operand_metadata
                            // would have set mem_addr, but handle defensively)
                            self.state.out.emit_instr_rbp_reg("    leaq", slot.0, &tmp_reg);
                        }
                    } else {
                        // Non-alloca: slot holds a pointer, load it
                        self.state.out.emit_instr_rbp_reg("    movq", slot.0, &tmp_reg);
                    }
                    op.mem_addr = format!("(%{})", tmp_reg);
                    return true;
                }
            }
            Operand::Const(c) => {
                // Constant address (e.g., from volatile MMIO reads like readl() in the kernel).
                // Copy propagation can replace Value operands with Const when the address
                // was originally a compile-time constant (e.g., fix_to_virt(X) + offset).
                // Load the constant address into a scratch register for indirect addressing.
                if let Some(addr) = c.to_i64() {
                    let tmp_reg = self.assign_scratch_reg(&AsmOperandKind::GpReg, excluded);
                    self.state.out.emit_instr_imm_reg("    movabsq", addr, &tmp_reg);
                    op.mem_addr = format!("(%{})", tmp_reg);
                    return true;
                }
            }
        }
        false
    }

    fn assign_scratch_reg(&mut self, kind: &AsmOperandKind, excluded: &[String]) -> String {
        if matches!(kind, AsmOperandKind::FpReg) {
            // Skip XMM registers that are claimed by clobbers or specific constraints.
            // Only xmm0-xmm15 are valid without AVX-512; xmm16+ requires EVEX encoding.
            loop {
                let idx = self.asm_xmm_scratch_idx;
                self.asm_xmm_scratch_idx += 1;
                if idx >= X86_XMM_SCRATCH.len() {
                    // All 16 XMM registers exhausted in the linear scan.
                    // Wrap around and pick the first non-excluded register.
                    // This allows register reuse between operands, which is
                    // acceptable for inline asm (inputs read before outputs written).
                    for r in X86_XMM_SCRATCH {
                        if !excluded.iter().any(|e| e == *r) {
                            return r.to_string();
                        }
                    }
                    // Every XMM register is excluded (extremely unlikely).
                    return "xmm0".to_string();
                }
                let reg = X86_XMM_SCRATCH[idx].to_string();
                if !excluded.iter().any(|e| e == &reg) {
                    return reg;
                }
            }
        } else if matches!(kind, AsmOperandKind::QReg) {
            // "Q" constraint: only rax/rbx/rcx/rdx have high-byte forms (%ah/%bh/%ch/%dh).
            // These are the only registers valid for the %h modifier in inline asm templates.
            for reg_name in &["rax", "rbx", "rcx", "rdx"] {
                if !excluded.iter().any(|e| e == *reg_name) {
                    return reg_name.to_string();
                }
            }
            // Fallback: all four legacy regs are excluded; use rax anyway.
            // This shouldn't happen in practice with correct inline asm.
            "rax".to_string()
        } else {
            // Skip registers that are claimed by specific-register constraints.
            // x86-64 GP registers: rcx, rdx, rsi, rdi, r8-r15 (no r16+).
            loop {
                let idx = self.asm_scratch_idx;
                self.asm_scratch_idx += 1;
                let reg = if idx < X86_GP_SCRATCH.len() {
                    X86_GP_SCRATCH[idx].to_string()
                } else {
                    let extra = idx - X86_GP_SCRATCH.len();
                    let gp_extra = extra + 12; // r12, r13, r14, r15
                    if gp_extra > 15 {
                        // All GP registers exhausted; fall back to rcx.
                        return "rcx".to_string();
                    }
                    format!("r{}", gp_extra)
                };
                if !excluded.iter().any(|e| e == &reg) {
                    return reg;
                }
            }
        }
    }

    fn load_input_to_reg(&mut self, op: &AsmOperand, val: &Operand, _constraint: &str) {
        let reg = &op.reg;
        let ty = op.operand_type;

        // x87 FPU stack: load value from memory onto the x87 stack with fld
        if matches!(op.kind, AsmOperandKind::X87St0 | AsmOperandKind::X87St1) {
            match val {
                Operand::Value(v) => {
                    if let Some(slot) = self.state.get_slot(v.0) {
                        let fld_instr = match ty {
                            IrType::F32 => "flds",
                            IrType::F128 => "fldt",
                            _ => "fldl", // F64 and default
                        };
                        self.state.emit_fmt(format_args!("    {} {}(%rbp)", fld_instr, slot.0));
                    }
                }
                Operand::Const(c) => {
                    // x87 can't load immediates directly; materialize via GP reg + stack scratch.
                    // Use the bits of the float constant as an integer, store to a scratch
                    // location on the stack, then fld from it. We use subq/addq to allocate
                    // scratch space instead of push/pop to avoid the peephole optimizer
                    // incorrectly eliminating the stack adjustment as a dead push/pop pair.
                    let bits = match ty {
                        IrType::F32 => {
                            let f = c.to_f64().unwrap_or(0.0) as f32;
                            f.to_bits() as u64
                        }
                        _ => {
                            let f = c.to_f64().unwrap_or(0.0);
                            f.to_bits()
                        }
                    };
                    self.state.emit("    subq $8, %rsp");
                    self.state.out.emit_instr_imm_reg("    movabsq", bits as i64, "rax");
                    self.state.emit("    movq %rax, (%rsp)");
                    let fld_instr = match ty {
                        IrType::F32 => "flds",
                        _ => "fldl",
                    };
                    self.state.emit_fmt(format_args!("    {} (%rsp)", fld_instr));
                    self.state.emit("    addq $8, %rsp");
                }
            }
            return;
        }

        let is_xmm = reg.starts_with("xmm");

        if is_xmm {
            // XMM register: use SSE/AVX load instructions
            match val {
                Operand::Value(v) => {
                    if let Some(slot) = self.state.get_slot(v.0) {
                        if self.state.is_alloca(v.0) {
                            // Alloca: data lives directly in the stack slot.
                            // Use movdqu (128-bit, unaligned-safe) for vector types,
                            // movss/movsd for scalar floats.
                            let load_instr = match ty {
                                IrType::F32 => "movss",
                                IrType::F64 => "movsd",
                                _ => "movdqu",
                            };
                            self.state.emit_fmt(format_args!("    {} {}(%rbp), %{}", load_instr, slot.0, reg));
                        } else {
                            let load_instr = match ty {
                                IrType::F32 => "movss",
                                _ => "movsd",
                            };
                            self.state.emit_fmt(format_args!("    {} {}(%rbp), %{}", load_instr, slot.0, reg));
                        }
                    }
                }
                Operand::Const(_) => {
                    // TODO: non-zero float constants need to be materialized via
                    // memory (e.g., load from .rodata). For now, zero the register.
                    // In practice, inline asm "x" inputs are almost always variables.
                    self.state.out.emit_instr_reg_reg("    xorpd", reg, reg);
                }
            }
            return;
        }

        match val {
            Operand::Const(c) => {
                // Extract IEEE 754 bit pattern for float constants.
                // to_i64() returns None for F32/F64, so use to_bits() instead.
                let imm = match c {
                    IrConst::F32(v) => v.to_bits() as i64,
                    IrConst::F64(v) => v.to_bits() as i64,
                    _ => c.to_i64().unwrap_or(0),
                };
                // When the register is a sub-64-bit name (e.g., "esi" from
                // `register uint32_t val asm("esi")`), we must use instructions
                // compatible with that register width. movabsq/xorq require
                // 64-bit register names; use the 64-bit equivalent instead.
                // Always derive reg64 and reg32 from the canonical 64-bit form
                // to handle 8-bit (al), 16-bit (ax), and 32-bit (eax) names.
                let reg64 = Self::reg_to_64(reg);
                let is_subreg = &*reg64 != reg;
                let effective_reg: &str = if is_subreg { &reg64 } else { reg };
                let reg32 = Self::reg_to_32(effective_reg);
                if imm == 0 {
                    // Use xorl with 32-bit register: shorter encoding and
                    // breaks dependencies, matching the codebase convention.
                    self.state.out.emit_instr_reg_reg("    xorl", &reg32, &reg32);
                } else if imm >= i32::MIN as i64 && imm <= i32::MAX as i64 {
                    self.state.out.emit_instr_imm_reg("    movq", imm, effective_reg);
                } else if imm >= 0 && imm <= u32::MAX as i64 {
                    // Value fits in unsigned 32-bit: use movl which zero-extends
                    // to 64 bits.
                    self.state.out.emit_instr_imm_reg("    movl", imm, &reg32);
                } else {
                    self.state.out.emit_instr_imm_reg("    movabsq", imm, effective_reg);
                }
            }
            Operand::Value(v) => {
                if let Some(slot) = self.state.get_slot(v.0) {
                    if self.state.is_alloca(v.0) {
                        // Alloca values represent the stack address of the variable.
                        // Use LEA to compute the address, not MOV which would
                        // load the contents stored at that address.
                        // Must use 64-bit register for LEA since stack addresses
                        // are 64 bits. If `reg` is a sub-64-bit name (e.g., "edi"
                        // from `register uint32_t x asm("edi")`), convert to the
                        // 64-bit equivalent to avoid truncating the address.
                        let reg64 = Self::reg_to_64(reg);
                        let lea_reg: &str = if &*reg64 != reg { &reg64 } else { reg };
                        if let Some(align) = self.state.alloca_over_align(v.0) {
                            self.state.out.emit_instr_rbp_reg("    leaq", slot.0, lea_reg);
                            self.state.out.emit_instr_imm_reg("    addq", (align - 1) as i64, lea_reg);
                            self.state.out.emit_instr_imm_reg("    andq", -(align as i64), lea_reg);
                        } else {
                            self.state.out.emit_instr_rbp_reg("    leaq", slot.0, lea_reg);
                        }
                    } else {
                        // Non-alloca values: load the value from the stack slot.
                        // Use type-appropriate load to avoid reading garbage from
                        // stack slots of smaller-than-8-byte variables.
                        let load_instr = Self::mov_load_for_type(ty);
                        let dest_reg_str = if matches!(ty, IrType::U32 | IrType::F32) {
                            format!("%{}", Self::reg_to_32(reg))
                        } else {
                            format!("%{}", reg)
                        };
                        self.state.emit_fmt(format_args!("    {} {}(%rbp), {}", load_instr, slot.0, dest_reg_str));
                    }
                }
            }
        }
    }

    fn preload_readwrite_output(&mut self, op: &AsmOperand, ptr: &Value) {
        // x87 FPU stack: preload with fld (same as input loading)
        if matches!(op.kind, AsmOperandKind::X87St0 | AsmOperandKind::X87St1) {
            let ty = op.operand_type;
            if let Some(slot) = self.state.get_slot(ptr.0) {
                let fld_instr = match ty {
                    IrType::F32 => "flds",
                    IrType::F128 => "fldt",
                    _ => "fldl",
                };
                if self.state.is_alloca(ptr.0) {
                    self.state.emit_fmt(format_args!("    {} {}(%rbp)", fld_instr, slot.0));
                } else {
                    let scratch = "rcx";
                    self.state.out.emit_instr_reg("    pushq", scratch);
                    self.state.out.emit_instr_rbp_reg("    movq", slot.0, scratch);
                    self.state.emit_fmt(format_args!("    {} (%{})", fld_instr, scratch));
                    self.state.out.emit_instr_reg("    popq", scratch);
                }
            }
            return;
        }
        let reg = &op.reg;
        let ty = op.operand_type;
        let is_xmm = reg.starts_with("xmm");
        if let Some(slot) = self.state.get_slot(ptr.0) {
            if self.state.is_alloca(ptr.0) {
                // Alloca: stack slot IS the variable's storage — load directly
                if is_xmm {
                    // Use movdqu for vector types (full 128-bit, unaligned-safe),
                    // movss/movsd for scalar floats.
                    let load_instr = match ty {
                        IrType::F32 => "movss",
                        IrType::F64 => "movsd",
                        _ => "movdqu",
                    };
                    self.state.emit_fmt(format_args!("    {} {}(%rbp), %{}", load_instr, slot.0, reg));
                } else {
                    let load_instr = Self::mov_load_for_type(ty);
                    let dest_reg = match ty {
                        IrType::U32 | IrType::F32 => format!("%{}", Self::reg_to_32(reg)),
                        _ => format!("%{}", reg),
                    };
                    self.state.emit_fmt(format_args!("    {} {}(%rbp), {}", load_instr, slot.0, dest_reg));
                }
            } else {
                // Non-alloca: stack slot holds a pointer — do indirect load
                self.state.out.emit_instr_rbp_reg("    movq", slot.0, reg);
                if is_xmm {
                    let load_instr = match ty {
                        IrType::F32 => "movss",
                        _ => "movsd",
                    };
                    self.state.emit_fmt(format_args!("    {} (%{}), %{}", load_instr, reg, reg));
                } else {
                    let load_instr = Self::mov_load_for_type(ty);
                    let dest_reg = match ty {
                        IrType::U32 | IrType::F32 => format!("%{}", Self::reg_to_32(reg)),
                        _ => format!("%{}", reg),
                    };
                    self.state.emit_fmt(format_args!("    {} (%{}), {}", load_instr, reg, dest_reg));
                }
            }
        }
    }

    fn substitute_template_line(&self, line: &str, operands: &[AsmOperand], gcc_to_internal: &[usize], operand_types: &[IrType], goto_labels: &[(String, BlockId)]) -> String {
        // Build the parallel arrays that substitute_x86_asm_operands expects
        let op_regs: Vec<String> = operands.iter().map(|o| o.reg.clone()).collect();
        let op_names: Vec<Option<String>> = operands.iter().map(|o| o.name.clone()).collect();
        let op_is_memory: Vec<bool> = operands.iter().map(|o| matches!(o.kind, AsmOperandKind::Memory)).collect();
        let op_mem_addrs: Vec<String> = operands.iter().map(|o| {
            if o.seg_prefix.is_empty() {
                o.mem_addr.clone()
            } else {
                format!("{}{}", o.seg_prefix, o.mem_addr)
            }
        }).collect();
        let op_imm_values: Vec<Option<i64>> = operands.iter().map(|o| o.imm_value).collect();
        let op_imm_symbols: Vec<Option<String>> = operands.iter().map(|o| o.imm_symbol.clone()).collect();

        // Build operand type array for register size selection
        let total = operands.len();
        let mut op_types: Vec<IrType> = vec![IrType::I64; total];
        for (i, ty) in operand_types.iter().enumerate() {
            if i < total { op_types[i] = *ty; }
        }
        // Inherit types for tied operands
        for (i, op) in operands.iter().enumerate() {
            if let AsmOperandKind::Tied(tied_to) = &op.kind {
                if *tied_to < op_types.len() && i < op_types.len() {
                    op_types[i] = op_types[*tied_to];
                }
            }
        }

        Self::substitute_x86_asm_operands(line, &op_regs, &op_names, &op_is_memory, &op_mem_addrs, &op_types, gcc_to_internal, goto_labels, &op_imm_values, &op_imm_symbols)
    }

    fn store_output_from_reg(&mut self, op: &AsmOperand, ptr: &Value, _constraint: &str, _all_output_regs: &[&str]) {
        if matches!(op.kind, AsmOperandKind::Memory) {
            return;
        }
        // x87 FPU stack outputs: store using fstp (store and pop)
        // The shared framework calls store_output_from_reg for outputs in order (index 0 first).
        // For "=t" (st(0)), fstp pops the top. For "=u" (st(1)), after st(0) was popped,
        // the old st(1) is now st(0), so another fstp stores it correctly.
        if matches!(op.kind, AsmOperandKind::X87St0 | AsmOperandKind::X87St1) {
            if let Some(slot) = self.state.get_slot(ptr.0) {
                let ty = op.operand_type;
                let fstp_instr = match ty {
                    IrType::F32 => "fstps",
                    IrType::F128 => "fstpt",
                    _ => "fstpl", // F64 and default
                };
                if self.state.is_alloca(ptr.0) {
                    self.state.emit_fmt(format_args!("    {} {}(%rbp)", fstp_instr, slot.0));
                } else {
                    // Non-alloca: slot holds a pointer, store through it
                    let scratch = "rcx";
                    self.state.out.emit_instr_reg("    pushq", scratch);
                    self.state.out.emit_instr_rbp_reg("    movq", slot.0, scratch);
                    self.state.emit_fmt(format_args!("    {} (%{})", fstp_instr, scratch));
                    self.state.out.emit_instr_reg("    popq", scratch);
                }
            }
            return;
        }
        // Handle =@cc<cond> condition code outputs: emit SETcc + movzbl
        if let AsmOperandKind::ConditionCode(ref cond) = op.kind {
            let reg = &op.reg;
            let reg8 = Self::reg_to_8l(reg);
            // Map GCC condition suffix to x86 SETcc instruction suffix
            let x86_cond = Self::gcc_cc_to_x86(cond);
            self.state.emit_fmt(format_args!("    set{} %{}", x86_cond, reg8));
            self.state.out.emit_instr_reg_reg("    movzbl", &reg8, &Self::reg_to_32(reg));
            // Store the result (0 or 1) to the output variable
            if let Some(slot) = self.state.get_slot(ptr.0) {
                let ty = op.operand_type;
                if self.state.is_alloca(ptr.0) {
                    let store_instr = Self::mov_store_for_type(ty);
                    let src_reg = match ty {
                        IrType::I8 | IrType::U8 => format!("%{}", Self::reg_to_8l(reg)),
                        IrType::I16 | IrType::U16 => format!("%{}", Self::reg_to_16(reg)),
                        IrType::I32 | IrType::U32 | IrType::F32 => format!("%{}", Self::reg_to_32(reg)),
                        _ => format!("%{}", reg),
                    };
                    self.state.emit_fmt(format_args!("    {} {}, {}(%rbp)", store_instr, src_reg, slot.0));
                } else {
                    let scratch = if reg != "rcx" { "rcx" } else { "rdx" };
                    self.state.out.emit_instr_reg("    pushq", scratch);
                    self.state.out.emit_instr_rbp_reg("    movq", slot.0, scratch);
                    let store_instr = Self::mov_store_for_type(ty);
                    let src_reg = match ty {
                        IrType::I8 | IrType::U8 => format!("%{}", Self::reg_to_8l(reg)),
                        IrType::I16 | IrType::U16 => format!("%{}", Self::reg_to_16(reg)),
                        IrType::I32 | IrType::U32 | IrType::F32 => format!("%{}", Self::reg_to_32(reg)),
                        _ => format!("%{}", reg),
                    };
                    self.state.emit_fmt(format_args!("    {} {}, (%{})", store_instr, src_reg, scratch));
                    self.state.out.emit_instr_reg("    popq", scratch);
                }
            }
            return;
        }
        let reg = &op.reg;
        let ty = op.operand_type;
        let is_xmm = reg.starts_with("xmm");
        if let Some(slot) = self.state.get_slot(ptr.0) {
            if is_xmm {
                // XMM register: use SSE/AVX store instructions
                if self.state.is_alloca(ptr.0) {
                    // Alloca: vector data lives directly in the stack slot.
                    // Use movdqu for a full 128-bit store for vector types
                    // (unaligned-safe since stack allocas may not be 16-byte aligned).
                    // Scalar floats use movss/movsd.
                    let store_instr = match ty {
                        IrType::F32 => "movss",
                        IrType::F64 => "movsd",
                        _ => "movdqu",
                    };
                    self.state.emit_fmt(format_args!("    {} %{}, {}(%rbp)", store_instr, reg, slot.0));
                } else {
                    let store_instr = match ty {
                        IrType::F32 => "movss",
                        _ => "movsd",
                    };
                    let scratch = "rcx";
                    self.state.out.emit_instr_reg("    pushq", scratch);
                    self.state.out.emit_instr_rbp_reg("    movq", slot.0, scratch);
                    self.state.emit_fmt(format_args!("    {} %{}, (%{})", store_instr, reg, scratch));
                    self.state.out.emit_instr_reg("    popq", scratch);
                }
            } else if self.state.is_alloca(ptr.0) {
                // Alloca: store directly to the stack slot with type-appropriate size
                let store_instr = Self::mov_store_for_type(ty);
                let src_reg = match ty {
                    IrType::I8 | IrType::U8 => format!("%{}", Self::reg_to_8l(reg)),
                    IrType::I16 | IrType::U16 => format!("%{}", Self::reg_to_16(reg)),
                    IrType::I32 | IrType::U32 | IrType::F32 => format!("%{}", Self::reg_to_32(reg)),
                    _ => format!("%{}", reg),
                };
                self.state.emit_fmt(format_args!("    {} {}, {}(%rbp)", store_instr, src_reg, slot.0));
            } else {
                // Non-alloca: slot holds a pointer, store through it.
                let scratch = if reg != "rcx" { "rcx" } else { "rdx" };
                self.state.out.emit_instr_reg("    pushq", scratch);
                self.state.out.emit_instr_rbp_reg("    movq", slot.0, scratch);
                let store_instr = Self::mov_store_for_type(ty);
                let src_reg = match ty {
                    IrType::I8 | IrType::U8 => format!("%{}", Self::reg_to_8l(reg)),
                    IrType::I16 | IrType::U16 => format!("%{}", Self::reg_to_16(reg)),
                    IrType::I32 | IrType::U32 | IrType::F32 => format!("%{}", Self::reg_to_32(reg)),
                    _ => format!("%{}", reg),
                };
                self.state.emit_fmt(format_args!("    {} {}, (%{})", store_instr, src_reg, scratch));
                self.state.out.emit_instr_reg("    popq", scratch);
            }
        }
    }

    fn reset_scratch_state(&mut self) {
        self.asm_scratch_idx = 0;
        self.asm_xmm_scratch_idx = 0;
    }
}
